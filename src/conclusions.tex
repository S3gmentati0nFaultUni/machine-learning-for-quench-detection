\chapter{Conclusions}
\label{chp:conclusion}
In this thesis we have proposed a solution to the problem of identifying quench events and
localizing them within the magnet structure using the harmonic decomposition of magnetic field. We
have provided a very strong solution for the first, and we obtained very promising initial results
for the second. During the project, we had different ideas to branch out and further explore the
fascinating realm of machine learning applied to superconducting magnet physics.

To achieve higher performance on \dts\ for \qlp, we can do further analysis on the models we
identified so far; or we can opt for a more powerful model and opt for \emph{post-hoc}
explainability~\cite{retzlaff2024}. While in the first case it's only a matter of experimentation,
in the second case we would need more data, but the result of such an analysis wouldn't necessarily
yield highly explainable results.

This work sets a path in the field of explainable automatic recognition of quench events, but we
explored one facet of the field, by doing \emph{post-mortem} analysis of the quadrupolar HO magnet.
During development, we came up with many different declinations, of the problem at hand, that we
could tackle.

\subsubsection{Analysis of the decision rules and general detection model}
In the project we have identified highly explainable models, capable of solving \qrp\ and \qlp\
effectively. The obvious next step, is to analyze the models and abstract useful rules, capable of
explaining quench events and which harmonics are more important to do localization. These rules could be used:
\begin{itemize}
	\item To create new quench antennae or quench protection systems capable of efficiently
	      detecting quenches within magnets. This can only be done if we know which are the
	      harmonics that best describe the event.
	\item To Support the magnet design process.
\end{itemize}

\subsubsection{Fuzzification of quench-event description}
For both our problems the quench-event was described by a binary value, if $0$ then the coil(s)
remained in the superconducting state, if $1$, then a quench event was measured. An interesting option would be to change the description of quenches,
moving to a representation of the label as a real number in the interval $[0, 1]$.

By fuzzifying the labels we could leverage a higher expressivity, highlighting different
behaviors: characterizing the rapidity of a transition to the normal-conducting state, or correlate
the quench event with the strength of the precursor (which is an instantaneous rise in the measured
voltage which might also pass undetected). The ones above are just examples, we could use this new labelling system to explain
different aspects of the magnet. All these descriptions would have to be inspected by a
superconducting magnet design expert to identify the correlations which are more interesting to be
investigated.

\subsubsection{Analysis of Voltage data}
In the original experiment two different measures were taken on the magnets: magnetic field measures (the
ones we used throughout this thesis) and voltage taps. Voltage taps are used to measure the voltage
in different positions inside the magnet, these taps are then connected to the electronics of the
\textsc{qps} (for more information about the voltage measurement system see \Cref{chp:problem}).

The magnetic measurement system is much slower compared to the voltage measurement system; this
means that we have much more data at our disposal that has not yet been used. At the time of writing
we are starting a cooperation between Milan University, INFN LASA and Naples University to build
high-performance models trained on such data.

\subsubsection{High performance quench localization}
High-performance models for online quench detection and localization are a different area of
research (quite active at the moment~\cite{hoang2021, zhou2021, einstein2023}).

Many are the alternatives: the \svcs\ explored in this thesis, neural networks or anomaly detection
via autoencoders. Clearly, pursuing such path forces us to invalid the explainability property we
granted throughout the project, unless explainable-AI models were used.

\subsubsection{Clustering-based preprocessing for classification}
Another technique that could be worth exploring is Clustering. We saw in the last chapter how the
samples were very nicely distributed in bidimensional space, specifically for attributes \an\ and
\cnmod.

As we said in \Cref{sec:qlp-cluster}, we could likely use clustering as a preprocessing step and then
extrapolate the results using highly explainable models like \dts\ and \rfs. We could also do a
first run of $k$-means clustering and then finalize classification using $k$-NN. Both approaches
would be extremely viable, at the expense, once more, of explainability (since we use $k$-means and
\pca, which are inherently non-explainable).




