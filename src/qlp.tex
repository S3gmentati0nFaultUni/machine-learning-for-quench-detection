\chapter{Quench Localization Problem (\qlp)}
\label{chp:qlp}
This chapter is dedicated to some initial results for the Quench Localization Problem (\qlp), we will discuss:
\begin{inparaenum}[(i)]
	\item some of the preprocessing we have done, specific for the labels,
	\item some preliminary results obtained by clustering, and finally,
	\item the results obtained, just like we did in \Cref{sec:results-qrp}
\end{inparaenum}.
\section{Data preprocessing}
\label{sec:qlp-preprocessing}
As we said in \Cref{chp:problem}, the main difference between \qlp\ and \qrp\ is the number of
classes. While \qrp\ is a binary classification problem, \qlp\ is a multiclass-classification
problem. As was already stated in \Cref{chp:problem}, the expected outcome is a $4$ bit binary array, each of the bits represents the state of one of the coils in the magnet ($1$ if the coil quenched, $0$ if it is in normal working conditions). In this section, we will concentrate on the considerations arisen from the analysis of the new labels, and the visualization of the samples in bidimensional space.

\subsubsection{\an}
\Cref{fig:an-lcorr-qlp} shows the correlation with the labels, \an\ is correlated very strongly with
coils $0$ and $2$, but is less suited to explain the behavior of coils $1$ and $3$. If we remind
ourselves of the correlation existing among the harmonics, originally shown in \Cref{fig:an-corr}, we can see that:
\begin{itemize}
	\item Contrarily to what we discovered in \Cref{sec:qrp-preprocessing}, \an[2]\ is not
	      fundamental to explain the expected results;
	\item The harmonics containing more information are the odd ones (\an[1], \an[3], \an[5],
	      \ldots). This means that we will be able to only take one from the bunch, due to the
	      strong correlation among odd-numbered harmonics (cfr. \Cref{fig:an-corr}).
\end{itemize}
A potential dataset could be constructed using a primary odd harmonic (like \an[1]\ or \an[3]), the
harmonic performing the best among \an[4], \an[8]\ and \an[12]\ (which are all strongly correlated,
cfr. \Cref{fig:an-corr}), and finally, \an[2]\ and another high-order harmonic could be beneficial. Due to
the very low correlation between the \an\ harmonics and the labels for coil $1$ and $3$ we didn't really
consider it as an attribute worth exploring.
\begin{figure}[!ht]
	% Font size = 70
	\centering
	\begin{subfigure}{0.49\linewidth}
		\includegraphics[width=\linewidth]{img/qlp_corr/An_coil0.png}
		\subcaption{Correlation with coil $0$}
	\end{subfigure}
	\begin{subfigure}{0.49\linewidth}
		\includegraphics[width=\linewidth]{img/qlp_corr/An_coil1.png}
		\subcaption{Correlation with coil $1$}
	\end{subfigure}
	\begin{subfigure}{0.49\linewidth}
		\includegraphics[width=\linewidth]{img/qlp_corr/An_coil2.png}
		\subcaption{Correlation with coil $2$}
	\end{subfigure}
	\begin{subfigure}{0.49\linewidth}
		\includegraphics[width=\linewidth]{img/qlp_corr/An_coil3.png}
		\subcaption{Correlation with coil $3$}
	\end{subfigure}
	\caption{Correlation between the harmonics of the \an\ attribute and the labels for \qlp.}
	\label{fig:an-lcorr-qlp}
\end{figure}

In figure \Cref{fig:an-coilq-dist} we visualized the distribution of samples for \an\ in
bidimensional space, after a round of \pca\ dimensionality reduction (moving from $15$ to $2$
dimensions). Sub-figure (a) labels data based on the number of quenched coils associated to the
sample. We can evidently identify a series of clusters, characterized by a high degree of purity, in
\Cref{sec:qlp-cluster} we are going to discuss the clustering approach on this specific attribute.
In sub-figure (b), we labelled the data based on which coil was quenched. The division of the labels
is not as neat as in sub-figure (a), which lead us to think that clustering could be used as a
preprocessing step to then have another machine learning model work on the clustered data to
predict quench localization.

While sub-figure (b) doesn't tell us everything we need to know, we can see that coil $1$ and coil
$3$ are fairly mixed together, furthermore both of them are usually involved in multi-coil quenches.
These hints are giving us another possible reason why the attribute is less-than-ideal to
predict quench localization on coils $3$ and $1$ \Cref{fig:an-lcorr-qlp}.

\begin{figure}[!ht]
	% Font size = 40
	\centering
	\begin{subfigure}{0.8\linewidth}
		\centering
		\includegraphics[width=\linewidth]{img/quench_dist_qlp/single_vs_multiple_An.png}
		\subcaption{}
	\end{subfigure}
	\begin{subfigure}{0.8\linewidth}
		\centering
		\includegraphics[width=\linewidth]{img/quench_dist_qlp_an.png}
		\subcaption{}
	\end{subfigure}
	\caption{Visualization of the \an\ attribute, the data was plotted after a run of \pca\
		dimensionality reduction. Sub-figure (a) highlights the samples based on how many quenches
		are associated to the specific sample $\{0, 1, \text{many}\}$. Sub-figure (b) highlights the
		samples based on the specific coil quenched $\{\text{None}, 0, 1, 2, 3\}$.}\label{fig:an-coilq-dist}
\end{figure}

\subsubsection{\bn}
While experimenting with \qrp, we discovered that \bn\ was the attribute that performed the least
among the ones available. We expected similar results for \qlp, but from preprocessing alone we
could not understand whether the attribute promised good or bad performance.

\Cref{fig:bn-lcorr-qlp} highlights the correlation between \bn\ harmonics and the label associated
to each coil (as described in \Cref{chp:problem}). The attribute, similarly to \an, sees a pattern
of strong correlations between its odd-numbered harmonics and coils $1$ and $3$. Since the pattern
of sub-figures (b) and (d) is very similar to the one shown in sub-figures (a) and (c) of
\Cref{fig:an-lcorr-qlp}, we imagined that a plausible dataset composition would have followed rules
similar to the ones identified for \an.
\begin{figure}[!ht]
	% Font size = 70
	\centering
	\begin{subfigure}{0.49\linewidth}
		\includegraphics[width=\linewidth]{img/qlp_corr/Bn_coil0.png}
		\subcaption{Correlation with coil $0$}
	\end{subfigure}
	\begin{subfigure}{0.49\linewidth}
		\includegraphics[width=\linewidth]{img/qlp_corr/Bn_coil1.png}
		\subcaption{Correlation with coil $1$}
	\end{subfigure}
	\begin{subfigure}{0.49\linewidth}
		\includegraphics[width=\linewidth]{img/qlp_corr/Bn_coil2.png}
		\subcaption{Correlation with coil $2$}
	\end{subfigure}
	\begin{subfigure}{0.49\linewidth}
		\includegraphics[width=\linewidth]{img/qlp_corr/Bn_coil3.png}
		\subcaption{Correlation with coil $3$}
	\end{subfigure}
	\caption{Correlation between the harmonics of the \bn\ attribute and the labels for \qlp.}
	\label{fig:bn-lcorr-qlp}
\end{figure}

In \Cref{fig:bn-coilq-dist} we visualized the samples in \bn\ after a round of \pca. Independently of the sub-figure we consider it's clear that the level of
homogeneity of the samples belonging to the central cluster is very high. While this, alone, cannot
induce us to think that the performance of models based on \bn\ is going to be bad, we are drawn to
consider the possibility that \bn\ might not be a good enough attribute if taken on its own.
\begin{figure}[!ht]
	% Font size = 40
	\centering
	\begin{subfigure}{0.8\linewidth}
		\centering
		\includegraphics[width=\linewidth]{img/quench_dist_qlp/single_vs_multiple_Bn.png}
		\subcaption{}
	\end{subfigure}
	\begin{subfigure}{0.8\linewidth}
		\centering
		\includegraphics[width=\linewidth]{img/quench_dist_qlp_bn.png}
		\subcaption{}
	\end{subfigure}
	\caption{Visualization of the \bn\ attribute, the data was plotted after a run of \pca\
		dimensionality reduction. Sub-figure (a) highlights the samples based on how many quenches
		are associated to the specific sample $\{0, 1, \text{many}\}$. Sub-figure (b) highlights the
		samples based on the specific coil quenched $\{\text{None}, 0, 1, 2, 3\}$.}
	\label{fig:bn-coilq-dist}
\end{figure}

\subsubsection{\cnmod}
As we said in \Cref{chp:problem}, \cnmod\ was expected to be very informative for \qrp, but not for
\qlp. Interestingly, though, the correlation between the harmonics for the attribute and the labels
associated to each coil, shown in \Cref{fig:cnmod-lcorr-qlp}, seem to be telling a different story.
If we consider coils $0$ and $3$, we can see a very strong correlation between the harmonics and the
label, it's also worth noting that, in sub-figures (a) through (d), \cnmod[2]\ seems to be extremely
important to explain the value associated with the labels.

If we look back at the correlation matrix computed for \cnmod\ during the preprocessing for \qrp\
(cfr. \Cref{fig:cnmod-corr}) a sub-view of \cnmod\ could be built as follows:
\begin{itemize}
	\item For coil $0$, \cnmod[2] and adding a high-order candidate. While this would technically maximize the total correlation with the label, we have seen in \qrp\ that choosing \cnmod[2]\ was a choice that didn't pay. That is why we might select \cnmod[3] or \cnmod[6] in the end, accompanied by another high order harmonic like \cnmod[9] or \cnmod[11].
	\item For coil $3$, we could be taking harmonic \cnmod[1] and then using one or more from
	      \{\cnmod[4], \cnmod[5], \cnmod[6], \cnmod[7], \cnmod[8]\}; finally, if it leads to
	      better performance, we could also add another high-order harmonic like \cnmod[10].
\end{itemize}
\begin{figure}[!ht]
	% Font size = 70
	\centering
	\begin{subfigure}{0.49\linewidth}
		\includegraphics[width=\linewidth]{img/qlp_corr/Cnmod_coil0.png}
		\subcaption{Correlation with coil $0$}
	\end{subfigure}
	\begin{subfigure}{0.49\linewidth}
		\includegraphics[width=\linewidth]{img/qlp_corr/Cnmod_coil1.png}
		\subcaption{Correlation with coil $1$}
	\end{subfigure}
	\begin{subfigure}{0.49\linewidth}
		\includegraphics[width=\linewidth]{img/qlp_corr/Cnmod_coil2.png}
		\subcaption{Correlation with coil $2$}
	\end{subfigure}
	\begin{subfigure}{0.49\linewidth}
		\includegraphics[width=\linewidth]{img/qlp_corr/Cnmod_coil3.png}
		\subcaption{Correlation with coil $3$}
	\end{subfigure}
	\caption{Correlation between the harmonics of the \cnmod\ attribute and the labels for \qlp.}
	\label{fig:cnmod-lcorr-qlp}
\end{figure}

As we did for \an\ and \bn, we visualized \cnmod\ after a round of \pca\ dimensionality reduction.
\Cref{fig:cnmod-coilq-dist} shows a similar situation to \an, if we consider the specific case of
sub-figure (a), while the clusters are not as clean cut as the alternative, there is much more
separation of the classes compared to \bn. As far as single coil quench is concerned (cfr. sub-figure
(b)), there is clearly a high degree of homogeneity due to having many groups of points labelled
differently but extremely close together.
\begin{figure}[!ht]
	% Font size = 40
	\centering
	\begin{subfigure}{0.8\linewidth}
		\centering
		\includegraphics[width=\linewidth]{img/quench_dist_qlp/single_vs_multiple_Cnmod.png}
		\subcaption{}
	\end{subfigure}
	\begin{subfigure}{0.8\linewidth}
		\centering
		\includegraphics[width=\linewidth]{img/quench_dist_qlp_cnmod.png}
		\subcaption{}
	\end{subfigure}
	\caption{Visualization of the \cnmod\ attribute, the data was plotted after a run of \pca\
		dimensionality reduction. Sub-figure (a) highlights the samples based on how many quenches
		are associated to the specific sample $\{0, 1, \text{many}\}$. Sub-figure (b) highlights the
		samples based on the specific coil quenched $\{\text{None}, 0, 1, 2, 3\}$.}
	\label{fig:cnmod-coilq-dist}
\end{figure}

\subsubsection{\phin}
To close the section we are going to consider the \phin\ attribute. In \Cref{fig:phi-lcorr-qlp} we
plotted the cross-correlation between the harmonics and the labels. Due to how high the correlation
is between most harmonics and basically all the coils, we might be lead to think that this is the
best attribute available to us. Since the harmonics are also very weakly correlated among themselves
(cfr. \Cref{fig:phi-corr}), we just chose to not extract any features and keep the attribute as is.
On the other hand, the bidimensional visualization of the data is telling a different story (cfr.
\Cref{fig:phi-coilq-dist}), closer, as a matter of fact, to what we discovered for \bn.

\Cref{fig:phi-coilq-dist} plots the data after a round of \pca\ dimensionality reduction.
Independently of the sub-figure we consider, the sparsity and the homogeneity of the data makes it
extremely hard to define clusters with a high level of purity.
\begin{figure}[!ht]
	% Font size = 70
	\centering
	\begin{subfigure}{0.49\linewidth}
		\includegraphics[width=\linewidth]{img/qlp_corr/Phi_coil0.png}
		\subcaption{Correlation with coil $0$}
	\end{subfigure}
	\begin{subfigure}{0.49\linewidth}
		\includegraphics[width=\linewidth]{img/qlp_corr/Phi_coil1.png}
		\subcaption{Correlation with coil $1$}
	\end{subfigure}
	\begin{subfigure}{0.49\linewidth}
		\includegraphics[width=\linewidth]{img/qlp_corr/Phi_coil2.png}
		\subcaption{Correlation with coil $2$}
	\end{subfigure}
	\begin{subfigure}{0.49\linewidth}
		\includegraphics[width=\linewidth]{img/qlp_corr/Phi_coil3.png}
		\subcaption{Correlation with coil $3$}
	\end{subfigure}
	\caption{Correlation between the harmonics of the \phin\ attribute and the labels for \qlp.}
	\label{fig:phi-lcorr-qlp}
\end{figure}

\begin{figure}[!ht]
	% Font size = 40
	\centering
	\begin{subfigure}{0.8\linewidth}
		\centering
		\includegraphics[width=\linewidth]{img/quench_dist_qlp/single_vs_multiple_Phi.png}
		\subcaption{}
	\end{subfigure}
	\begin{subfigure}{0.8\linewidth}
		\centering
		\includegraphics[width=\linewidth]{img/quench_dist_qlp_phi.png}
		\subcaption{}
	\end{subfigure}
	\caption{Visualization of the \phin\ attribute, the data was plotted after a run of \pca\
		dimensionality reduction. Sub-figure (a) highlights the samples based on how many quenches
		are associated to the specific sample $\{0, 1, \text{many}\}$. Sub-figure (b) highlights the
		samples based on the specific coil quenched $\{\text{None}, 0, 1, 2, 3\}$.}
	\label{fig:phi-coilq-dist}
\end{figure}

\section{First approaches using Clustering}
\label{sec:qlp-cluster}
While we were working on \qrp\ we discovered that \an\ and \cnmod\ data was scattered favorably in
bidimensional space. We hoped that clustering on these attributes would yield unexpected insights
for \qlp, giving us a very early glimpse of undiscovered patterns in the data.

We remark that the clustering algorithm is always working by using distance between samples,
therefore the pattern that it's capable of identifying is unknown to us. We need to give a meaning
to the cluster structure identified by the algorithm. We tried $k$-means clustering on all
attributes but we will only show relevant results. Data was plotted using \pca, but we also explored
other dimensionality reduction techniques, namely FastICA and Metric multiDimensional Scaling (MDS),
generally yielding inferior results.

\Cref{fig:4-means-results} shows results of a $4$-means clustering run on both attributes, we chose
to label the samples based on the number of quenched coils $\{0, 1, \text{many}\}$. The orange lines
in the figure represent the cluster borders, if the line is solid then it has finite size, otherwise
it has infinite size.

While the result for \an\ (sub-figure (a)) contains $3$ clusters that essentially have a high degree
of purity (the top for the magnet in normal operating conditions, the left and right for quench
events). The central cluster mixes quench and non-quench data, making it less useful for our
classification needs. If we consider \cnmod (sub-figure (b)), while it's far from perfect, all
clusters have a decent degree of purity.
\begin{figure}[!ht]
	\centering
	\begin{subfigure}{0.8\linewidth}
		\centering
		\includegraphics[width=\linewidth]{img/clustering_an_qlp_4c.png}
		\caption{}
	\end{subfigure}
	\begin{subfigure}{0.8\linewidth}
		\centering
		\includegraphics[width=\linewidth]{img/clustering_cnmod_qlp_4c.png}
		\caption{}
	\end{subfigure}
	\caption{A run of $4$-means on attributes \an\ and \cnmod\ after a round of \pca\
		dimensionality reduction. As we can see using $4$ clusters leads to a suboptimal division of
		the samples, while \cnmod\ has clusters with a bit more purity (at least quench-events are
		not mixed with non-quenches).} \label{fig:4-means-results}
\end{figure}

According to the clustering-specific performance metrics (Silhouette and \textsc{db} scores, cfr.
\Cref{sec:kmeans}) using $4$ clusters represented for \an\ and \cnmod the best possible solution.
As we discussed above, though, this choice lead to suboptimal results, especially for \an.  We
experimented with higher cluster counts, while \cnmod\ gets visually worse. The algorithms splits
the south-east cluster of \Cref{fig:cnmod-coilq-dist} (sub-figure (b)) in different sub-clusters; a
preliminary explanation could be that it's trying to separate quench events belonging to different
coils (based on the per-coil quench visualization, cfr. \Cref{fig:cnmod-coilq-dist} (sub-figure (b))).

A run of $8$-means on \an\ generates clusters with a high level of purity. \Cref{fig:clustering-an} plots the result, the clusters identified by the algorithm are very clearly trying to separate three classes of samples: one associated to the magnet being in normal working condition, one associated to the magnet containing \emph{at most} one quenched coil, and finally, one associated to the magnet containing \emph{at least} one quenched coil.
\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.8\linewidth]{img/clustering_an_qlp_8c.png}
	\caption{The results of a run of $8$-means on the sole attribute \an, after a round of \pca
		dimensionality reduction. As we can see all clusters have a high purity and only a handful of points have been misclassified.}\label{fig:clustering-an}
\end{figure}

Using clustering we achieved very interesting results and, while clustering alone might not be
enough as a classifier; we could still use it as a preprocessor for the models we saw in
\Cref{chp:qrp}. Alternatively, we could think of correcting misclassification errors by running an instance of $k$-nn on top of $k$-means.

\section{Results}
After our brief exploration of the possibilities offered by clustering algorithms, we moved back to
supervised machine learning algorithms to solve \qlp. Our first approach was to do a na\"ive
extension of the models used to solve \qrp, to the multiclass-classification environment. We didn't
have high expectations for such models since the number of labels changed from $1$ to $4$ (cfr.
\Cref{chp:problem}). Making the problem much more difficult than the original one.

Using \dts\ to solve a multiclass-classification problem yielded more complicated estimators. With a
maximum depth $\in [5, 10]$, a high number of nodes, and an accuracy score close to $60 - 70\%$. This rough performance
number was obtained by micro averaging the accuracy of the classifier for every class; in other
words, the resulting metric is the average of the metric computed on each class, weighed on the cardinality of the class.

Since the extension of \dts\ did not suit our expectations, we chose to change approach. Our new
idea was simply to turn \qlp\ into a binary classification problem.

We first had to define a variation of \qrp\ meant for single coils instead of the whole quadrupole. This variation, which we will call \qrp-$i$, is capable of predicting, given the harmonic decomposition of magnetic field, whether a certain coil quenched $1$ or not $0$.

The $4$ bit label vector associated to each sample for \qlp\ can be divided in the $4$ constituting
bits; each one stating whether one of the $4$ coils has quenched or not. Being able to predict this vector means
solving \qrp-$i$ for all $i \in \{0, 1, 2, 3\}$.

We solved \qrp-$i$, instead of reusing the best classifiers found for \qrp, because the function we found with
\qrp links the harmonic decomposition of magnetic field to quench events in a quadrupole. \qrp-$i$
uses the same data to pose a different question, it might sound like a very subtle change, but it's
an important one.

In the following sections we will retrace our steps in \Cref{chp:qrp} and discuss the obtained results for the various models.

\subsection{Decision trees}
To keep the discussion concise, we will be describing the structure of the best models and
concentrate on the performance of the aggregate. \Cref{eq:hamming} shows the metric we used to
compute the performance for the aggregate model, which contains one classifier per coil. We
denominated the metric 'Hamming score' ($\hs$ in the following); it computes the Hamming distance between the expected and the predicted vectors, and then normalizes it.
\begin{equation}
	\label{eq:hamming}
	\hs = 1 - \frac{d_h(\hat{y}, y)}{4}
\end{equation}

\Cref{fig:bdts-qlp} visualizes the performance metrics obtained in the outer loop of \ncv\ for the best single
tree trained on each coil. The metrics are quite close to each other, with the only clear outlier
being the model trained on coil $3$.

We condensed the most important information in \Cref{tbl:tree-description}. All the trees have a
maximum depth of $5$ and the number of internal nodes and leaves is always acceptable. While the
trees built on \an\ remain on the smaller side, the trees used for coil $1$ and $3$ are larger (with
the one built on \cnmod\ being the more complex at $15$ total nodes).
\begin{figure}[!ht]
	\centering
	\includegraphics[width=\linewidth]{img/best_dts_qlp.png}
	\caption{Comparing the performance of the best model built on every coil, independently of
		the dataset used.} \label{fig:bdts-qlp}
\end{figure}

\begin{table}[!ht]
	\caption{Description of the best \dt\ for every coil.}\label{tbl:tree-description}

	\bigskip
	\setlength{\tabcolsep}{6pt}
	\centering
	\begin{tabular}{lcccc}
		\toprule
		\textbf{}                           & \textbf{Coil 0}  & \textbf{Coil 1}          & \textbf{Coil 2}          & \textbf{Coil 3}
		\\
		\midrule
		\textsc{attribute}                  & \an              & \bn                      & \an                      & \cnmod                          \\
		\multirow{2}{*}{\textsc{harmonics}} & \an[2], \an[3]   & \bn[3], \bn[4], \bn[10], & \an[1], \an[2], \an[12], & \cnmod[1], \cnmod[6], \cnmod[7] \\
		                                    &
		                                    & \bn[11], \bn[13] & \an[15]                  &                                                            \\
		\textsc{depth}                      & 3                & 5
		                                    & 3                & 5                                                                                     \\
		\textsc{N internal nodes}           & 4                & 5
		                                    & 3                & 8                                                                                     \\
		\textsc{N leaves}                   & 5                & 6
		                                    & 4                & 7                                                                                     \\
		\textsc{N nodes}                    & 9                & 11
		                                    & 7                & 15                                                                                    \\
		\bottomrule
	\end{tabular}
\end{table}

In \Cref{fig:dt-qlp-hs}, we plotted the performance of the aggregate model, obtained as we
did when testing \tas\ for \qrp. A run of $5$-fold \cv, for each sample we:
\begin{inparaenum}[(i)]
\item predicted the value of the $4$ bits,
\item put them in a vector and computed $\hs$,
\item plotted the point.
\end{inparaenum}
\Cref{fig:dt-qlp-hs} shows the $\hs$ for every sample of ever fold. The ensemble makes at most $2$
errors, in most cases it just does one single error, since $\hs$ always stays around $0.75$. By
averaging the performance over all folds, we get $0.967$ with a standard deviation of $0.051$. In
\Cref{tbl:dt-errors} we computed the number of errors done by every model on all the testing folds
to get a sense of which models made more errors. Finding a better model for coil $3$ would increase
ensemble performance since this aggregate is not like \tas; this entails that the stronger model is not making up
for the errors made by the weaker classifiers (which is actually one of the reasons why \rfs\ and
\tas\ are more powerful than single trees, cfr. \Cref{sec:el}).
\begin{figure}[!ht]
	\centering
	\includegraphics[width=\linewidth]{img/best_dts_hs.png}
	\caption{The Hamming score computed on every fold (for the $50$ samples therein contained), the lower reaching is the line the lower is going to be the accuracy.} \label{fig:dt-qlp-hs}
\end{figure}

Testing the performance of the aggregate on the final blind-test causes $4$ total errors ($4$ bits
were misclassified out of the $116$ total). If we average the Hamming score on the $29$ samples, we get higher
performance on average $0.966$, compared to training, and a standard deviation of $0.086$.
\begin{table}[!ht]
	\caption{Each column contains the errors done by the model built on the specific coil for
	the specific testing fold, each fold contains $50$ samples.}\label{tbl:dt-errors}

	\bigskip
	\setlength{\tabcolsep}{6pt}
	\centering
	\begin{tabular}{ccccc}
		\toprule
		\textbf{}                     & \textbf{Coil 0}    & \textbf{Coil 1} & \textbf{Coil 2} & \textbf{Coil 3}
		\\
		\midrule
		\textsc{Fold 0}         & $2$           & $0$           & $0$            & $5$            \\
		\textsc{Fold 1}         & $2$		& $3$		& $0$            & $3$		  \\
		\textsc{Fold 2}		& $0$           & $0$		& $1$            & $4$            \\
		\textsc{Fold 3}         & $2$           & $2$ 		& $2$            & $1$            \\
		\textsc{Fold 4}         & $0$		& $3$           & $0$            & $3$            \\
		\midrule
		\textsc{total errors}	& $6$		& $8$		& $3$		& $16$
		\\
		\bottomrule
	\end{tabular}
\end{table}

\subsection{Random forests}
Similarly to what we just did for \dts\ we also evaluated the performance of a model containing one forest per coil, we expected higher performance compared to the ones just obtained on \dts, at the expense of a lower explainability.

\Cref{fig:brfs-qlp} shows the performance for the best \rfs\ built on a mix of different attributes
(each taken in its entirety). Performance metrics are better, but we are not far off from the numbers obtained in the case of \dts\ (cfr. \Cref{fig:bdts-qlp}). The weakest learner, in this case as well, was the one built on coil $3$.
\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{img/best_rfs_qlp.png}
	\caption{The best random forest trained on a sub-view or mix of sub-views, for every coil.
		Since all models have been plotted without indicating the harmonic content, it means
		that they are using all the available harmonics.}\label{fig:brfs-qlp}
\end{figure}

\Cref{tbl:forest-description} describes the four best forests. The depth, Number of internal nodes and number of leaves has been
averaged on all the trees to give an idea of what the average tree would look like. We also included
the $5$ most important harmonics for each forest, to have a sense of which harmonics counted the
most in the estimator. A couple of aspects stood up to us:
\begin{itemize}
	\item The forests are very complicated, they contain many trees and can be quite deep. The
		most complicated of them all is the one constructed on coil $3$, which reaches an
		average number of nodes of $15$ for $10$ trees.
	\item The model dedicated to $0$ is trained on a dataset using attributes \an, \cnmod\ and
		\phin. Despite this, the $5$ most important harmonics are all taken from \an.
\end{itemize}
\begin{table}[!ht]
	\caption{Description of the best \rf\ for every coil.}\label{tbl:forest-description}

	\bigskip
	\setlength{\tabcolsep}{6pt}
	\centering
	\begin{tabular}{lcccc}
		\toprule
		\textbf{}                     & \textbf{Coil 0}    & \textbf{Coil 1} & \textbf{Coil 2} & \textbf{Coil 3}
		\\
		\midrule
		\textsc{attribute}            & \an, \cnmod, \phin & \bn, \phin
		                              & \an                & \phin                                               \\
			\multirow{2}{*}{\textsc{harmonics}} & \an[3], \an[10], \an[11]   & \phin[6],
			\bn[13], \bn[3], & \an[3], \an[7], \an[13], & \phin[6], \phin[11], \phin[13] \\
							    & \an[7], \an[9]
							    & \bn[9], \bn[5] & \an[10], \an[12]
							    &\phin[1], \phin[10]                                                            \\
		\textsc{N estimators}         & $5$                & $10$            & $10$            & $10$            \\
		\textsc{avg. depth}            & $2.6$              & $3.7$
		                              & $3  $              & $4  $                                               \\
		\textsc{avg. N int. nodes} & $2.6$              & $4.4$
		                              & $3.5$              & $7.9$                                               \\
		\textsc{avg. N leaves}         & $4.5$              & $5.4$
		                              & $4.9$              & $8.9$                                               \\
		\bottomrule
	\end{tabular}
\end{table}
The excessive complexity of these models lead us to abandon the model early because the slight
performance increase over \dts\ wasn't really worth the complexity jump.

\subsubsection{Support Vector Classifiers}
As a benchmark model we used, once again, \svcs. If we train it without constraints (one or more sub-views for
different datasets), the results are that we can aim at a very high level of performance if we
forget about explainability. Performance have been plotted in \Cref{fig:bsvcs-qlp}, the metrics were
taken from the outer \cv\ loop and the models chosen were the best for each specific coil.
\begin{figure}[!ht]
	\centering
	\includegraphics[width=\textwidth]{img/best_svcs_qlp.png}
	\caption{Performance metrics for the best \svcs\ trained on every coil, all attributes
		have been taken with their full harmonic content.} \label{fig:bsvcs-qlp}
\end{figure}
There are a couple of very interesting takeaways:
\begin{itemize}
	\item The best \svc\ trained on coil $0$ is based on attribute \an. This is exactly what we
	      expected since the very beginning of this chapter.
	\item The best models for coils $1$ and $3$, were trained on \bn\ and another dataset (\cnmod\ for $1$
	      and \phin\ for $3$). This is interesting for two reasons:
	      \begin{inparaenum}[(i)]
		      \item \bn\ is actually the best attribute for the two coils, as we saw in the analysis
		      we did in the beginning of the chapter,
		      \item the \svc\ trained on the sole \bn\ performed worse than the alternatives. This
		      lead us to believe that, like in the case of \qrp, \bn\ alone doesn't contain
		      enough information
	      \end{inparaenum}.
	\item Interestingly, while \cnmod\ didn't perform well enough in our tests on trees for coil
	      $2$, in the case of \svc\ the model was picked over \an.
\end{itemize}

In \Cref{fig:svc-qlp-hs} we plotted $\hs$ for the $5$ cross validation folds, as we did for \dts, in
\Cref{tbl:svc-err} we indicated the error done by each model on the $5$ folds and the total for each
model. Clearly, while the \svc\ trained on \an performs way better than the others, the models
mounted on the remaining coils have more modest performance. The average $\hs$ score for the \svc\
is $0.994$ and its standard deviation is $0.006$.
\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{img/svc_qlp_hs.png}
	\caption{The Hamming score computed on every fold (for the $50$ samples therein contained),
	fir \svcs. The lower reaching is the line the lower is going to be the accuracy.}
	\label{fig:svc-qlp-hs}
\end{figure}
\begin{table}[!ht]
	\caption{Description of the best \rf\ for every coil.}\label{tbl:svc-err}

	\bigskip
	\setlength{\tabcolsep}{6pt}
	\centering
	\begin{tabular}{ccccc}
		\toprule
		\textbf{}                     & \textbf{Coil 0}    & \textbf{Coil 1} & \textbf{Coil 2} & \textbf{Coil 3}
		\\
		\midrule
		\textsc{Fold 0}         & $0$           & $0$           & $0$            & $0$            \\
		\textsc{Fold 1}         & $0$		& $0$		& $2$            & $0$		  \\
		\textsc{Fold 2}		& $0$           & $0$		& $0$            & $1$            \\
		\textsc{Fold 3}         & $1$           & $1$ 		& $1$            & $0$            \\
		\textsc{Fold 4}         & $0$		& $0$           & $0$            & $0$            \\
		\midrule
		\textsc{total errors}	& $1$		& $1$		& $3$		& $1$
		\\
		\bottomrule
	\end{tabular}
\end{table}

Despite the aggregate of \svcs\ performs much better than the aggregate of \dts, we see that both
models make $4$ mistakes on the blind-test set \db. This leads us to say that we found a solid
solution in \dts that could be further improved by using more advanced techniques.






