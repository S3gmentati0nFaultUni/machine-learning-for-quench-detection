@book{slimani2022superconducting,
	title = {Superconducting Materials: Fundamentals, Synthesis and
	         Applications},
	author = {Slimani, Y. and Hannachi, E.},
	isbn = {9789811912115},
	url = {https://books.google.it/books?id=HBVuEAAAQBAJ},
	year = {2022},
	publisher = {Springer Nature Singapore},
}

@book{tsukerman2020compendium,
	title = {Compendium On Electromagnetic Analysis - From Electrostatics To
	         Photonics: Fundamentals And Applications For Physicists And
	         Engineers (In 5 Volumes)},
	author = {Tsukerman, I.},
	isbn = {9789813270183},
	url = {https://books.google.it/books?id=VPgTEAAAQBAJ},
	year = {2020},
	publisher = {World Scientific Publishing Company},
}

@book{fokker1925physica,
	title = {Physica: Nederlandsch tijdschrift voor natuurkunde},
	author = {Fokker, A.D. and Oosterhuis, E. and van der Pol, B.},
	number = {v. 5},
	lccn = {31004996},
	url = {https://books.google.it/books?id=qhUNAQAAIAAJ},
	year = {1925},
	publisher = {M. Nijhoff},
}

@book{fujita-theory-HTS,
	author = {Fujita, Shigeji and Godoy, Salvador},
	year = {2003},
	month = {01},
	pages = {},
	title = {Theory of High Temperature Superconductivity},
	isbn = {978-1-4020-0149-9},
	journal = {Theory of High Temperature Superconductivity: , Fundamental
	           Theories of Physics, Volume 121. ISBN 978-1-4020-0149-9.
	           Kluwer Academic Publishers, 2003},
	doi = {10.1007/0-306-48216-9},
}

@book{ZhouZhi-Hua2021ML,
	abstract = {Machine Learning},
	author = {Zhou, Zhi-Hua},
	address = {Singapore},
	copyright = {These electronic books are licensed by OhioLINK and may be
	             under copyright protection. Please see the Acceptable Use
	             Guidelines for more information, or contact your librarian.},
	edition = {1st Edition 2021},
	isbn = {9811519668},
	keywords = {Computer Science ; Data Mining and Knowledge Discovery ;
	            Machine Learning ; Mathematics of Computing},
	language = {eng},
	publisher = {Springer Singapore},
	title = {Machine Learning},
	year = {2021},
}

@book{Rebala2019,
	author = "Rebala, Gopinath and Ravi, Ajay and Churiwala, Sanjay",
	title = "Machine Learning Definition and Basics",
	bookTitle = "An Introduction to Machine Learning",
	year = "2019",
	publisher = "Springer International Publishing",
	address = "Cham",
	pages = "1--17",
	abstract = "Machine learning algorithms have shown great promise in
	            providing solutions to complex problems. Some of the
	            applications we use every day from searching the Internet to
	            speech recognition are examples of tremendous strides made in
	            realizing the promise of machine learning.",
	isbn = "978-3-030-15729-6",
	doi = "10.1007/978-3-030-15729-6_1",
	url = "https://doi.org/10.1007/978-3-030-15729-6_1",
}

@book{Cunningham2008,
	author = "Cunningham, P{\'a}draig and Cord, Matthieu and Delany, Sarah
	          Jane",
	editor = "Cord, Matthieu and Cunningham, P{\'a}draig",
	title = "Supervised Learning",
	bookTitle = "Machine Learning Techniques for Multimedia: Case Studies on
	             Organization and Retrieval",
	year = "2008",
	publisher = "Springer Berlin Heidelberg",
	address = "Berlin, Heidelberg",
	pages = "21--49",
	abstract = "Supervised learning accounts for a lot of research activity
	            in machine learning and many supervised learning techniques
	            have found application in the processing of multimedia
	            content. The defining characteristic of supervised learning
	            is the availability of annotated training data. The name
	            invokes the idea of a `supervisor' that instructs the
	            learning system on the labels to associate with training
	            examples. Typically these labels are class labels in
	            classification problems. Supervised learning algorithms
	            induce models from these training data and these models can
	            be used to classify other unlabelled data. In this chapter we
	            ground or analysis of supervised learning on the theory of
	            risk minimization. We provide an overview of support vector
	            machines and nearest neighbour classifiers{\textasciitilde}--
	            probably the two most popular supervised learning techniques
	            employed in multimedia research.",
	isbn = "978-3-540-75171-7",
	doi = "10.1007/978-3-540-75171-7_2",
	url = "https://doi.org/10.1007/978-3-540-75171-7_2",
}

@book{gray2011entropy,
	title = {Entropy and Information Theory},
	author = {Gray, R.M.},
	isbn = {9781441979704},
	series = {SpringerLink : B{\"u}cher},
	url = {https://books.google.it/books?id=wdSOqgVbdRcC},
	year = {2011},
	publisher = {Springer US},
}

@book{quinlan2014c4,
	title = {C4.5: Programs for Machine Learning},
	author = {Quinlan, J.R.},
	isbn = {9780080500584},
	series = {Ebrary online},
	url = {https://books.google.it/books?id=b3ujBQAAQBAJ},
	year = {2014},
	publisher = {Morgan Kaufmann},
}

@book{breiman1984classification,
	title = {Classification and Regression Trees},
	author = {Breiman, L.},
	isbn = {9780534980535},
	lccn = {83019708},
	series = {(The Wadsworth statistics / probability series)},
	url = {https://books.google.de/books?id=uxPvAAAAMAAJ},
	year = {1984},
	publisher = {Wadsworth International Group},
}
	
@book{fisher1996learning,
	title = {Learning from Data: Artificial Intelligence and Statistics V},
	author = {Fisher, D. and Lenz, H.J.},
	isbn = {9780387947365},
	lccn = {96011794},
	series = {Artificial intelligence and statistics},
	url = {https://books.google.de/books?id=cmf_l21oje8C},
	year = {1996},
	publisher = {Springer New York},
}

@book{bramer2007principles,
	title = {Principles of Data Mining},
	author = {Bramer, M.},
	isbn = {9781846287664},
	lccn = {2007922358},
	series = {Undergraduate Topics in Computer Science},
	url = {https://books.google.de/books?id=xVW7NslHNHsC},
	year = {2007},
	publisher = {Springer London},
}

@book{learning-with-kernels,
	author = {Schölkopf, Bernhard and Smola, Alexander J.},
	title = {Learning with Kernels: Support Vector Machines, Regularization,
	         Optimization, and Beyond},
	publisher = {The MIT Press},
	year = {2001},
	month = {12},
	abstract = {A comprehensive introduction to Support Vector Machines and
	            related kernel methods.In the 1990s, a new type of learning
	            algorithm was developed, based on results from statistical
	            learning theory: the Support Vector Machine (SVM). This gave
	            rise to a new class of theoretically elegant learning
	            machines that use a central concept of SVMs—-kernels—for a
	            number of learning tasks. Kernel machines provide a modular
	            framework that can be adapted to different tasks and domains
	            by the choice of the kernel function and the base algorithm.
	            They are replacing neural networks in a variety of fields,
	            including engineering, information retrieval, and
	            bioinformatics.Learning with Kernels provides an introduction
	            to SVMs and related kernel methods. Although the book begins
	            with the basics, it also includes the latest research. It
	            provides all of the concepts necessary to enable a reader
	            equipped with some basic mathematical knowledge to enter the
	            world of machine learning using theoretically well-founded
	            yet easy-to-use kernel algorithms and to understand and apply
	            the powerful algorithms that have been developed over the
	            last few years.},
	isbn = {9780262256933},
	doi = {10.7551/mitpress/4175.001.0001},
	url = {https://doi.org/10.7551/mitpress/4175.001.0001},
}


@inbook{kkt1951,
	author = "Kuhn, Harold W. and Tucker, Albert W.",
	editor = "Giorgi, Giorgio and Kjeldsen, Tinne Hoff",
	title = "Nonlinear Programming",
	bookTitle = "Traces and Emergence of Nonlinear Programming",
	year = "2014",
	publisher = "Springer Basel",
	address = "Basel",
	pages = "247--258",
	abstract = "Linear programming deals with problems such as (see [ 4], [
	            5]): to maximize a linear function {\$}{\$} {\backslash}rm g{
	            \{}x{\}}{\backslash}equiv {\backslash}sum {\{}c{\_}{\{}i{\}}x
	            {\_}{\{}i{\}}{\}} {\backslash}; {\backslash}rm {\{}of{\}} {
	            \backslash}; n {\backslash};{\backslash}rm{\{}real {
	            \backslash}; variables{\}} {\backslash}; x{\_}{\{}1{\}},...,x
	            {\_}{\{}n{\}} {\$}{\$}(forming a vector x) constrained by m +
	            n linear inequalities.",
	isbn = "978-3-0348-0439-4",
	doi = "10.1007/978-3-0348-0439-4_11",
	url = "https://doi.org/10.1007/978-3-0348-0439-4_11",
}

@article{invention-superconductivity,
	author = {van Delft, Dirk and Kes, Peter},
	title = {The discovery of superconductivity},
	journal = {Physics Today},
	volume = {63},
	number = {9},
	pages = {38-43},
	year = {2010},
	month = {09},
	abstract = {In a triumphant report to the Royal Netherlands Academy of
	            Arts and Sciences (KNAW), Kamerlingh Onnes documented his
	            achievement in great detail. Therefore it is remarkable that
	            reliable details about his serendipitous discovery of
	            superconductivity three years later have been hard to come
	            by. Lack of information has led to speculations about the
	            discovery. In particular, it has perpetuated an apocryphal
	            tale about the role played by a sleepy young apprentice in
	            Kamerlingh Onnes’s lab. That tale was treated as established
	            fact in a September 1996 Physics Today article by Jacobus de
	            Nobel (page 40). There have even been rumors of the possible
	            disappearance of Kamerlingh Onnes’s laboratory notebooks.},
	issn = {0031-9228},
	doi = {10.1063/1.3490499},
	url = {https://doi.org/10.1063/1.3490499},
}

@article{meissner1933,
	author = {Meissner, W. and Ochsenfeld, R.},
	title = {Ein neuer Effekt bei Eintritt der Supraleitf{\"a}higkeit},
	journal = {Naturwissenschaften},
	year = {1933},
	month = {Nov},
	day = {01},
	volume = {21},
	number = {44},
	pages = {787-788},
	issn = {1432-1904},
	doi = {10.1007/BF01504252},
	url = {https://doi.org/10.1007/BF01504252},
}

@article{london1935,
	title = { The electromagnetic equations of the supraconductor },
	author = { F. London and H. London },
	journal = { Proceedings of The Royal Society A: Mathematical, Physical
	           and Engineering Sciences },
	year = { 1935 },
	publisher = { The Royal Society },
	volume = { 149 },
	pages = { 71-88 },
	number = { 866 },
	doi = { 10.1098/RSPA.1935.0048 },
}

@article{polarization-magnetization,
	authors = {Carlo Andrea Gonano and Riccardo Enrico Zich and Marco
	           Mussetta},
	title = {DEFINITION FOR POLARIZATION P AND MAGNETIZATION M FULLY
	         CONSISTENT WITH MAXWELL'S EQUATIONS},
	volume = {64},
	journal = {Progress In Electromagnetics Research B},
	year = {2015},
	pages = {83-101},
	doi = {10.2528/PIERB15100606},
}

@article{abrikosov-vortices,
	title = {The magnetic properties of superconducting alloys},
	journal = {Journal of Physics and Chemistry of Solids},
	volume = {2},
	number = {3},
	pages = {199-208},
	year = {1957},
	issn = {0022-3697},
	doi = {https://doi.org/10.1016/0022-3697(57)90083-5},
	url = {
	       https://www.sciencedirect.com/science/article/pii/0022369757900835
	       },
	author = {A.A. Abrikosov},
	abstract = {A new explanation is proposed for the magnetic properties of
	            superconducting alloys based on the theory of Ginzburg and
	            Landau.(1) The structure of the penetrating field is found
	            for a superconductor and the relationship between induction
	            and field strength. The findings are compared with
	            experimental results.},
}

@article{mariotto2022,
	doi = {10.1088/1361-6668/ac39e8},
	url = {https://dx.doi.org/10.1088/1361-6668/ac39e8},
	year = {2021},
	month = {nov},
	publisher = {IOP Publishing},
	volume = {35},
	number = {1},
	pages = {015006},
	author = {Mariotto, S and Sorbi, M},
	title = {Quench position reconstruction through harmonic field analysis
	         in superconducting magnets},
	journal = {Superconductor Science and Technology},
	abstract = {The performances of superconducting magnets for particle
	            accelerators are limited by instabilities or disturbances
	            which lead to the transition of the superconducting material
	            to the normal resistive state and the activation of the
	            quench protection system to prevent damage to the magnet. To
	            locate the position of the state transition, voltage taps or
	            quench antennas are the most commonly used technologies for
	            their reliability and accuracy. However, during the
	            production phase of a magnet, the number of voltage taps is
	            commonly reduced to simplify the construction process and
	            quench antennas are generally used only for dipoles or
	            quadrupoles to limit the antenna design complexity. To
	            increase the accuracy in the reconstruction of the quench
	            event position, a novel method, suitable for magnets with
	            independent superconducting coils and quench protected
	            without the use of quench heaters, is proposed in this paper.
	            This method, based on standard magnetic measurement
	            techniques for field harmonic analysis, can locate the
	            position of the superconductor transition inside the magnet
	            after the quench event when the magnet has been discharged.
	            Analyzing the not allowed harmonics produced in the field
	            quality at zero current, the position of the quenched coils
	            can be retrieved for any magnet orders without increasing the
	            complexity of the dedicated measurement technique.},
}

@article{mariotto2022-generic,
	author = {Mariotto, Samuele and De Matteis, Ernesto and Prioli, Marco
	          and Sorbi, Malou and Statera, Marco and Valente, Riccardo},
	year = {2022},
	month = {06},
	pages = {1-1},
	title = {Quench Localization in the High Order Corrector Magnets Using
	         the Harmonic Field Method},
	volume = {32},
	journal = {IEEE Transactions on Applied Superconductivity},
	doi = {10.1109/TASC.2022.3158626},
}

@article{Cyrot1973,
	doi = {10.1088/0034-4885/36/2/001},
	url = {https://dx.doi.org/10.1088/0034-4885/36/2/001},
	year = {1973},
	month = {feb},
	publisher = {},
	volume = {36},
	number = {2},
	pages = {103},
	author = {M Cyrot},
	title = {Ginzburg-Landau theory for superconductors},
	journal = {Reports on Progress in Physics},
	abstract = {In this review the author describes how the simple
	            Ginzburg-Landau approach lies in the heart of the general
	            theory of superconductors. The reader is introduced to the
	            handling of the theory and to the numerous possibilities of
	            applications. A general free energy functional for a
	            superconductor is given and the different cases where it can
	            be reduced to a Ginzburg-Landau form or to a simple
	            generalization of this form is studied. It is emphasized that
	            applications are not restricted to thermo-dynamical ones as
	            the Ginzburg-Landau approach can be used in the calculation
	            of dissipative phenomena. The possibility of an extension to
	            the time-dependent phenomena is discussed in detail to
	            present the difficult problems which arise in that case. The
	            gapless regime of type II superconductors is given as an
	            example.},
}

@article{diamantini2023typeiiisuperconductivity,
	title = {Type III superconductivity},
	author = {M. C. Diamantini and C. A. Trugenberger and Sheng-Zong Chen
	          and Yu-Jung Lu and Chi-Te Liang and V. M. Vinokur},
	year = {2023},
	eprint = {2303.14673},
	archivePrefix = {arXiv},
	primaryClass = {cond-mat.supr-con},
	url = {https://arxiv.org/abs/2303.14673},
}

@article{pal2010handwritten,
	title = {Handwritten English character recognition using neural network},
	author = {Pal, Anita and Singh, Dayashankar},
	journal = {International Journal of Computer science \&amp;
	           Communication},
	volume = {1},
	number = {2},
	pages = {141--144},
	year = {2010},
}

@article{overfitting-dt-erblin,
	AUTHOR = {Erblin Halabaku, Eliot Bytyçi},
	TITLE = {Overfitting in Machine Learning: A Comparative Analysis of
	         Decision Trees and Random Forests},
	JOURNAL = {Intelligent Automation \& Soft Computing},
	VOLUME = {39},
	YEAR = {2024},
	NUMBER = {6},
	PAGES = {987--1006},
	URL = {http://www.techscience.com/iasc/v39n6/59139},
	ISSN = {2326-005X},
	ABSTRACT = {Machine learning has emerged as a pivotal tool in
	            deciphering and managing this excess of information in an era
	            of abundant data. This paper presents a comprehensive
	            analysis of machine learning algorithms, focusing on the
	            structure and efficacy of random forests in mitigating
	            overfitting—a prevalent issue in decision tree models. It
	            also introduces a novel approach to enhancing decision tree
	            performance through an optimized pruning method called
	            Adaptive Cross-Validated Alpha CCP (ACV-CCP). This method
	            refines traditional cost complexity pruning by streamlining
	            the selection of the alpha parameter, leveraging
	            cross-validation within the pruning process to achieve a
	            reliable, computationally efficient alpha selection that
	            generalizes well to unseen data. By enhancing computational
	            efficiency and balancing model complexity, ACV-CCP allows
	            decision trees to maintain predictive accuracy while
	            minimizing overfitting, effectively narrowing the performance
	            gap between decision trees and random forests. Our findings
	            illustrate how ACV-CCP contributes to the robustness and
	            applicability of decision trees, providing a valuable
	            perspective on achieving computationally efficient and
	            generalized machine learning models.},
	DOI = {10.32604/iasc.2024.059429},
}

@article{platt1998,
	author = {Platt, John},
	year = {1998},
	month = {07},
	pages = {},
	title = {Sequential Minimal Optimization: A Fast Algorithm for Training
	         Support Vector Machines},
	volume = {208},
	journal = {Advances in Kernel Methods-Support Vector Learning},
}

@article{cover1965,
	author = {Cover, Thomas M.},
	journal = {IEEE Transactions on Electronic Computers},
	title = {Geometrical and Statistical Properties of Systems of Linear
	         Inequalities with Applications in Pattern Recognition},
	year = {1965},
	volume = {EC-14},
	number = {3},
	pages = {326-334},
	keywords = {Pattern recognition;Vectors;Application software;Boolean
	            functions;Geometry;History},
	doi = {10.1109/PGEC.1965.264137},
}

@inproceedings{nguyen2013,
	author = {Nguyen, Tan T. and Sanner, Scott},
	title = {Algorithms for direct 0-1 loss optimization in binary
	         classification},
	year = {2013},
	publisher = {JMLR.org},
	abstract = {While convex losses for binary classification are attractive
	            due to the existence of numerous (provably) efficient methods
	            for finding their global optima, they are sensitive to
	            outliers. On the other hand, while the nonconvex 0-1 loss is
	            robust to outliers, it is NP-hard to optimize and thus rarely
	            directly optimized in practice. In this paper, however, we do
	            just that: we explore a variety of practical methods for
	            direct (approximate) optimization of the 0-1 loss based on
	            branch and bound search, combinatorial search, and coordinate
	            descent on smooth, differentiable relaxations of 0-1 loss.
	            Empirically, we compare our proposed algorithms to logistic
	            regression, SVM, and the Bayes point machine showing that the
	            proposed 0-1 loss optimization algorithms perform at least as
	            well and offer a clear advantage in the presence of outliers.
	            To this end, we believe this work reiterates the importance
	            of 0-1 loss and its robustness properties while challenging
	            the notion that it is difficult to directly optimize.},
	booktitle = {Proceedings of the 30th International Conference on
	             International Conference on Machine Learning - Volume 28},
	pages = {III–1085–III–1093},
	location = {Atlanta, GA, USA},
	series = {ICML'13},
}

@article{Fowlkes1983,
	author = {E. B. Fowlkes and C. L. Mallows},
	title = {A Method for Comparing Two Hierarchical Clusterings},
	journal = {Journal of the American Statistical Association},
	volume = {78},
	number = {383},
	pages = {553--569},
	year = {1983},
	publisher = {ASA Website},
	doi = {10.1080/01621459.1983.10478008},
	URL = {
	       https://www.tandfonline.com/doi/abs/10.1080/01621459.1983.10478008
	       },
	eprint = {
	          https://www.tandfonline.com/doi/pdf/10.1080/01621459.1983.10478008
	          },
	abstract = { This article concerns the derivation and use of a measure
	            of similarity between two hierarchical clusterings. The
	            measure, Bk , is derived from the matching matrix, [mij ],
	            formed by cutting the two hierarchical trees and counting the
	            number of matching entries in the k clusters in each tree.
	            The mean and variance of Bk are determined under the
	            assumption that the margins of [mij ] are fixed. Thus, Bk
	            represents a collection of measures for k = 2, …, n – 1. (k,
	            Bk ) plots are found to be useful in portraying the
	            similarity of two clusterings. Bk is compared to other
	            measures of similarity proposed respectively by Baker (1974)
	            and Rand (1971). The use of (k, Bk ) plots for studying
	            clustering methods is explored by a series of Monte Carlo
	            sampling experiments. An example of the use of (k, Bk ) on
	            real data is given. },
}

@article{Rand1971,
	author = {William M. Rand},
	title = {Objective Criteria for the Evaluation of Clustering Methods},
	journal = {Journal of the American Statistical Association},
	volume = {66},
	number = {336},
	pages = {846--850},
	year = {1971},
	publisher = {ASA Website},
	doi = {10.1080/01621459.1971.10482356},
	URL = {
	       https://www.tandfonline.com/doi/abs/10.1080/01621459.1971.10482356
	       },
	eprint = {
	          https://www.tandfonline.com/doi/pdf/10.1080/01621459.1971.10482356
	          },
	abstract = { Many intuitively appealing methods have been suggested for
	            clustering data, however, interpretation of their results has
	            been hindered by the lack of objective criteria. This article
	            proposes several criteria which isolate specific aspects of
	            the performance of a method, such as its retrieval of
	            inherent structure, its sensitivity to resampling and the
	            stability of its results in the light of new data. These
	            criteria depend on a measure of similarity between two
	            different clusterings of the same set of data; the measure
	            essentially considers how each pair of data points is
	            assigned in each clustering. },
}

@article{bouldin1979,
	author = {Davies, David L. and Bouldin, Donald W.},
	journal = {IEEE Transactions on Pattern Analysis and Machine
	           Intelligence},
	title = {A Cluster Separation Measure},
	year = {1979},
	volume = {PAMI-1},
	number = {2},
	pages = {224-227},
	abstract = {A measure is presented which indicates the similarity of
	            clusters which are assumed to have a data density which is a
	            decreasing function of distance from a vector characteristic
	            of the cluster. The measure can be used to infer the
	            appropriateness of data partitions and can therefore be used
	            to compare relative appropriateness of various divisions of
	            the data. The measure does not depend on either the number of
	            clusters analyzed nor the method of partitioning of the data
	            and can be used to guide a cluster seeking algorithm.},
	keywords = {Dispersion;Density measurement;Algorithm design and
	            analysis;Clustering algorithms;Partitioning
	            algorithms;Multidimensional systems;Data analysis;Performance
	            analysis;Humans;Missiles;Cluster;data
	            partitions;multidimensional data analysis;parametric
	            clustering;partitions;similarity measure},
	doi = {10.1109/TPAMI.1979.4766909},
	ISSN = {1939-3539},
	month = {April},
}
