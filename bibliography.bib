@book{slimani2022superconducting,
	title = {Superconducting Materials: Fundamentals, Synthesis and
	         Applications},
	author = {Slimani, Y. and Hannachi, E.},
	isbn = {9789811912115},
	url = {https://books.google.it/books?id=HBVuEAAAQBAJ},
	year = {2022},
	publisher = {Springer Nature Singapore},
}

@book{tsukerman2020compendium,
	title = {Compendium On Electromagnetic Analysis - From Electrostatics To
	         Photonics: Fundamentals And Applications For Physicists And
	         Engineers (In 5 Volumes)},
	author = {Tsukerman, I.},
	isbn = {9789813270183},
	url = {https://books.google.it/books?id=VPgTEAAAQBAJ},
	year = {2020},
	publisher = {World Scientific Publishing Company},
}

@book{fokker1925physica,
	title = {Physica: Nederlandsch tijdschrift voor natuurkunde},
	author = {Fokker, A.D. and Oosterhuis, E. and van der Pol, B.},
	number = {v. 5},
	lccn = {31004996},
	url = {https://books.google.it/books?id=qhUNAQAAIAAJ},
	year = {1925},
	publisher = {M. Nijhoff},
}

@book{fujita-theory-HTS,
	author = {Fujita, Shigeji and Godoy, Salvador},
	year = {2003},
	month = {01},
	pages = {},
	title = {Theory of High Temperature Superconductivity},
	isbn = {978-1-4020-0149-9},
	journal = {Theory of High Temperature Superconductivity: , Fundamental
	           Theories of Physics, Volume 121. ISBN 978-1-4020-0149-9.
	           Kluwer Academic Publishers, 2003},
	doi = {10.1007/0-306-48216-9},
}

@book{ZhouZhi-Hua2021ML,
	abstract = {Machine Learning},
	author = {Zhou, Zhi-Hua},
	address = {Singapore},
	copyright = {These electronic books are licensed by OhioLINK and may be
	             under copyright protection. Please see the Acceptable Use
	             Guidelines for more information, or contact your librarian.},
	edition = {1st Edition 2021},
	isbn = {9811519668},
	keywords = {Computer Science ; Data Mining and Knowledge Discovery ;
	            Machine Learning ; Mathematics of Computing},
	language = {eng},
	publisher = {Springer Singapore},
	title = {Machine Learning},
	year = {2021},
}

@book{Rebala2019,
	author = "Rebala, Gopinath and Ravi, Ajay and Churiwala, Sanjay",
	title = "Machine Learning Definition and Basics",
	bookTitle = "An Introduction to Machine Learning",
	year = "2019",
	publisher = "Springer International Publishing",
	address = "Cham",
	pages = "1--17",
	abstract = "Machine learning algorithms have shown great promise in
	            providing solutions to complex problems. Some of the
	            applications we use every day from searching the Internet to
	            speech recognition are examples of tremendous strides made in
	            realizing the promise of machine learning.",
	isbn = "978-3-030-15729-6",
	doi = "10.1007/978-3-030-15729-6_1",
	url = "https://doi.org/10.1007/978-3-030-15729-6_1",
}

@book{Cunningham2008,
	author = "Cunningham, P{\'a}draig and Cord, Matthieu and Delany, Sarah
	          Jane",
	editor = "Cord, Matthieu and Cunningham, P{\'a}draig",
	title = "Supervised Learning",
	bookTitle = "Machine Learning Techniques for Multimedia: Case Studies on
	             Organization and Retrieval",
	year = "2008",
	publisher = "Springer Berlin Heidelberg",
	address = "Berlin, Heidelberg",
	pages = "21--49",
	abstract = "Supervised learning accounts for a lot of research activity
	            in machine learning and many supervised learning techniques
	            have found application in the processing of multimedia
	            content. The defining characteristic of supervised learning
	            is the availability of annotated training data. The name
	            invokes the idea of a `supervisor' that instructs the
	            learning system on the labels to associate with training
	            examples. Typically these labels are class labels in
	            classification problems. Supervised learning algorithms
	            induce models from these training data and these models can
	            be used to classify other unlabelled data. In this chapter we
	            ground or analysis of supervised learning on the theory of
	            risk minimization. We provide an overview of support vector
	            machines and nearest neighbour classifiers{\textasciitilde}--
	            probably the two most popular supervised learning techniques
	            employed in multimedia research.",
	isbn = "978-3-540-75171-7",
	doi = "10.1007/978-3-540-75171-7_2",
	url = "https://doi.org/10.1007/978-3-540-75171-7_2",
}

@book{gray2011entropy,
	title = {Entropy and Information Theory},
	author = {Gray, R.M.},
	isbn = {9781441979704},
	series = {SpringerLink : B{\"u}cher},
	url = {https://books.google.it/books?id=wdSOqgVbdRcC},
	year = {2011},
	publisher = {Springer US},
}

@book{quinlan2014c4,
	title = {C4.5: Programs for Machine Learning},
	author = {Quinlan, J.R.},
	isbn = {9780080500584},
	series = {Ebrary online},
	url = {https://books.google.it/books?id=b3ujBQAAQBAJ},
	year = {2014},
	publisher = {Morgan Kaufmann},
}

@book{breiman1984classification,
	title = {Classification and Regression Trees},
	author = {Breiman, L.},
	isbn = {9780534980535},
	lccn = {83019708},
	series = {(The Wadsworth statistics / probability series)},
	url = {https://books.google.de/books?id=uxPvAAAAMAAJ},
	year = {1984},
	publisher = {Wadsworth International Group},
}
	
@book{fisher1996learning,
	title = {Learning from Data: Artificial Intelligence and Statistics V},
	author = {Fisher, D. and Lenz, H.J.},
	isbn = {9780387947365},
	lccn = {96011794},
	series = {Artificial intelligence and statistics},
	url = {https://books.google.de/books?id=cmf_l21oje8C},
	year = {1996},
	publisher = {Springer New York},
}

@book{bramer2007principles,
	title = {Principles of Data Mining},
	author = {Bramer, M.},
	isbn = {9781846287664},
	lccn = {2007922358},
	series = {Undergraduate Topics in Computer Science},
	url = {https://books.google.de/books?id=xVW7NslHNHsC},
	year = {2007},
	publisher = {Springer London},
}

@book{learning-with-kernels,
	author = {Schölkopf, Bernhard and Smola, Alexander J.},
	title = {Learning with Kernels: Support Vector Machines, Regularization,
	         Optimization, and Beyond},
	publisher = {The MIT Press},
	year = {2001},
	month = {12},
	abstract = {A comprehensive introduction to Support Vector Machines and
	            related kernel methods.In the 1990s, a new type of learning
	            algorithm was developed, based on results from statistical
	            learning theory: the Support Vector Machine (SVM). This gave
	            rise to a new class of theoretically elegant learning
	            machines that use a central concept of SVMs—-kernels—for a
	            number of learning tasks. Kernel machines provide a modular
	            framework that can be adapted to different tasks and domains
	            by the choice of the kernel function and the base algorithm.
	            They are replacing neural networks in a variety of fields,
	            including engineering, information retrieval, and
	            bioinformatics.Learning with Kernels provides an introduction
	            to SVMs and related kernel methods. Although the book begins
	            with the basics, it also includes the latest research. It
	            provides all of the concepts necessary to enable a reader
	            equipped with some basic mathematical knowledge to enter the
	            world of machine learning using theoretically well-founded
	            yet easy-to-use kernel algorithms and to understand and apply
	            the powerful algorithms that have been developed over the
	            last few years.},
	isbn = {9780262256933},
	doi = {10.7551/mitpress/4175.001.0001},
	url = {https://doi.org/10.7551/mitpress/4175.001.0001},
}

@book{Nishok2024,
	title = {Machine Learning Algorithms},
	author = {Nishok, V.S. and Arunkumar, A. and Murthy, A. and Praveena, S.
	          },
	isbn = {9789348020901},
	url = {https://books.google.it/books?id=COpCEQAAQBAJ},
	year = {2024},
	publisher = {RK Publication},
}





@inbook{kkt1951,
	author = "Kuhn, Harold W. and Tucker, Albert W.",
	editor = "Giorgi, Giorgio and Kjeldsen, Tinne Hoff",
	title = "Nonlinear Programming",
	bookTitle = "Traces and Emergence of Nonlinear Programming",
	year = "2014",
	publisher = "Springer Basel",
	address = "Basel",
	pages = "247--258",
	abstract = "Linear programming deals with problems such as (see [ 4], [
	            5]): to maximize a linear function {\$}{\$} {\backslash}rm g{
	            \{}x{\}}{\backslash}equiv {\backslash}sum {\{}c{\_}{\{}i{\}}x
	            {\_}{\{}i{\}}{\}} {\backslash}; {\backslash}rm {\{}of{\}} {
	            \backslash}; n {\backslash};{\backslash}rm{\{}real {
	            \backslash}; variables{\}} {\backslash}; x{\_}{\{}1{\}},...,x
	            {\_}{\{}n{\}} {\$}{\$}(forming a vector x) constrained by m +
	            n linear inequalities.",
	isbn = "978-3-0348-0439-4",
	doi = "10.1007/978-3-0348-0439-4_11",
	url = "https://doi.org/10.1007/978-3-0348-0439-4_11",
}















@article{invention-superconductivity,
	author = {van Delft, Dirk and Kes, Peter},
	title = {The discovery of superconductivity},
	journal = {Physics Today},
	volume = {63},
	number = {9},
	pages = {38-43},
	year = {2010},
	month = {09},
	abstract = {In a triumphant report to the Royal Netherlands Academy of
	            Arts and Sciences (KNAW), Kamerlingh Onnes documented his
	            achievement in great detail. Therefore it is remarkable that
	            reliable details about his serendipitous discovery of
	            superconductivity three years later have been hard to come
	            by. Lack of information has led to speculations about the
	            discovery. In particular, it has perpetuated an apocryphal
	            tale about the role played by a sleepy young apprentice in
	            Kamerlingh Onnes’s lab. That tale was treated as established
	            fact in a September 1996 Physics Today article by Jacobus de
	            Nobel (page 40). There have even been rumors of the possible
	            disappearance of Kamerlingh Onnes’s laboratory notebooks.},
	issn = {0031-9228},
	doi = {10.1063/1.3490499},
	url = {https://doi.org/10.1063/1.3490499},
}

@article{meissner1933,
	author = {Meissner, W. and Ochsenfeld, R.},
	title = {Ein neuer Effekt bei Eintritt der Supraleitf{\"a}higkeit},
	journal = {Naturwissenschaften},
	year = {1933},
	month = {11},
	day = {01},
	volume = {21},
	number = {44},
	pages = {787-788},
	issn = {1432-1904},
	doi = {10.1007/BF01504252},
	url = {https://doi.org/10.1007/BF01504252},
}

@article{london1935,
	title = { The electromagnetic equations of the supraconductor },
	author = { F. London and H. London },
	journal = { Proceedings of The Royal Society A: Mathematical, Physical
	           and Engineering Sciences },
	year = { 1935 },
	publisher = { The Royal Society },
	volume = { 149 },
	pages = { 71-88 },
	number = { 866 },
	doi = { 10.1098/RSPA.1935.0048 },
}

@article{polarization-magnetization,
	authors = {Carlo Andrea Gonano and Riccardo Enrico Zich and Marco
	           Mussetta},
	title = {DEFINITION FOR POLARIZATION P AND MAGNETIZATION M FULLY
	         CONSISTENT WITH MAXWELL'S EQUATIONS},
	volume = {64},
	journal = {Progress In Electromagnetics Research B},
	year = {2015},
	pages = {83-101},
	doi = {10.2528/PIERB15100606},
}

@article{abrikosov-vortices,
	title = {The magnetic properties of superconducting alloys},
	journal = {Journal of Physics and Chemistry of Solids},
	volume = {2},
	number = {3},
	pages = {199-208},
	year = {1957},
	issn = {0022-3697},
	doi = {https://doi.org/10.1016/0022-3697(57)90083-5},
	url = {
	       https://www.sciencedirect.com/science/article/pii/0022369757900835
	       },
	author = {A.A. Abrikosov},
	abstract = {A new explanation is proposed for the magnetic properties of
	            superconducting alloys based on the theory of Ginzburg and
	            Landau.(1) The structure of the penetrating field is found
	            for a superconductor and the relationship between induction
	            and field strength. The findings are compared with
	            experimental results.},
}

@article{mariotto2022,
	doi = {10.1088/1361-6668/ac39e8},
	url = {https://dx.doi.org/10.1088/1361-6668/ac39e8},
	year = {2021},
	month = {11},
	publisher = {IOP Publishing},
	volume = {35},
	number = {1},
	pages = {015006},
	author = {Mariotto, S and Sorbi, M},
	title = {Quench position reconstruction through harmonic field analysis
	         in superconducting magnets},
	journal = {Superconductor Science and Technology},
	abstract = {The performances of superconducting magnets for particle
	            accelerators are limited by instabilities or disturbances
	            which lead to the transition of the superconducting material
	            to the normal resistive state and the activation of the
	            quench protection system to prevent damage to the magnet. To
	            locate the position of the state transition, voltage taps or
	            quench antennas are the most commonly used technologies for
	            their reliability and accuracy. However, during the
	            production phase of a magnet, the number of voltage taps is
	            commonly reduced to simplify the construction process and
	            quench antennas are generally used only for dipoles or
	            quadrupoles to limit the antenna design complexity. To
	            increase the accuracy in the reconstruction of the quench
	            event position, a novel method, suitable for magnets with
	            independent superconducting coils and quench protected
	            without the use of quench heaters, is proposed in this paper.
	            This method, based on standard magnetic measurement
	            techniques for field harmonic analysis, can locate the
	            position of the superconductor transition inside the magnet
	            after the quench event when the magnet has been discharged.
	            Analyzing the not allowed harmonics produced in the field
	            quality at zero current, the position of the quenched coils
	            can be retrieved for any magnet orders without increasing the
	            complexity of the dedicated measurement technique.},
}

@article{mariotto2022-generic,
	author = {Mariotto, Samuele and De Matteis, Ernesto and Prioli, Marco
	          and Sorbi, Malou and Statera, Marco and Valente, Riccardo},
	year = {2022},
	month = {06},
	pages = {1-1},
	title = {Quench Localization in the High Order Corrector Magnets Using
	         the Harmonic Field Method},
	volume = {32},
	journal = {IEEE Transactions on Applied Superconductivity},
	doi = {10.1109/TASC.2022.3158626},
}

@article{Cyrot1973,
	doi = {10.1088/0034-4885/36/2/001},
	url = {https://dx.doi.org/10.1088/0034-4885/36/2/001},
	year = {1973},
	month = {02},
	publisher = {},
	volume = {36},
	number = {2},
	pages = {103},
	author = {M Cyrot},
	title = {Ginzburg-Landau theory for superconductors},
	journal = {Reports on Progress in Physics},
	abstract = {In this review the author describes how the simple
	            Ginzburg-Landau approach lies in the heart of the general
	            theory of superconductors. The reader is introduced to the
	            handling of the theory and to the numerous possibilities of
	            applications. A general free energy functional for a
	            superconductor is given and the different cases where it can
	            be reduced to a Ginzburg-Landau form or to a simple
	            generalization of this form is studied. It is emphasized that
	            applications are not restricted to thermo-dynamical ones as
	            the Ginzburg-Landau approach can be used in the calculation
	            of dissipative phenomena. The possibility of an extension to
	            the time-dependent phenomena is discussed in detail to
	            present the difficult problems which arise in that case. The
	            gapless regime of type II superconductors is given as an
	            example.},
}

@article{diamantini2023typeiiisuperconductivity,
	title = {Type III superconductivity},
	author = {M. C. Diamantini and C. A. Trugenberger and Sheng-Zong Chen
	          and Yu-Jung Lu and Chi-Te Liang and V. M. Vinokur},
	year = {2023},
	eprint = {2303.14673},
	archivePrefix = {arXiv},
	primaryClass = {cond-mat.supr-con},
	url = {https://arxiv.org/abs/2303.14673},
}

@article{pal2010handwritten,
	title = {Handwritten English character recognition using neural network},
	author = {Pal, Anita and Singh, Dayashankar},
	journal = {International Journal of Computer science \&amp;
	           Communication},
	volume = {1},
	number = {2},
	pages = {141--144},
	year = {2010},
}

@article{overfitting-dt-erblin,
	AUTHOR = {Erblin Halabaku, Eliot Bytyçi},
	TITLE = {Overfitting in Machine Learning: A Comparative Analysis of
	         Decision Trees and Random Forests},
	JOURNAL = {Intelligent Automation \& Soft Computing},
	VOLUME = {39},
	YEAR = {2024},
	NUMBER = {6},
	PAGES = {987--1006},
	URL = {http://www.techscience.com/iasc/v39n6/59139},
	ISSN = {2326-005X},
	ABSTRACT = {Machine learning has emerged as a pivotal tool in
	            deciphering and managing this excess of information in an era
	            of abundant data. This paper presents a comprehensive
	            analysis of machine learning algorithms, focusing on the
	            structure and efficacy of random forests in mitigating
	            overfitting—a prevalent issue in decision tree models. It
	            also introduces a novel approach to enhancing decision tree
	            performance through an optimized pruning method called
	            Adaptive Cross-Validated Alpha CCP (ACV-CCP). This method
	            refines traditional cost complexity pruning by streamlining
	            the selection of the alpha parameter, leveraging
	            cross-validation within the pruning process to achieve a
	            reliable, computationally efficient alpha selection that
	            generalizes well to unseen data. By enhancing computational
	            efficiency and balancing model complexity, ACV-CCP allows
	            decision trees to maintain predictive accuracy while
	            minimizing overfitting, effectively narrowing the performance
	            gap between decision trees and random forests. Our findings
	            illustrate how ACV-CCP contributes to the robustness and
	            applicability of decision trees, providing a valuable
	            perspective on achieving computationally efficient and
	            generalized machine learning models.},
	DOI = {10.32604/iasc.2024.059429},
}

@article{platt1998,
	author = {Platt, John},
	year = {1998},
	month = {07},
	pages = {},
	title = {Sequential Minimal Optimization: A Fast Algorithm for Training
	         Support Vector Machines},
	volume = {208},
	journal = {Advances in Kernel Methods-Support Vector Learning},
}

@article{cover1965,
	author = {Cover, Thomas M.},
	journal = {IEEE Transactions on Electronic Computers},
	title = {Geometrical and Statistical Properties of Systems of Linear
	         Inequalities with Applications in Pattern Recognition},
	year = {1965},
	volume = {EC-14},
	number = {3},
	pages = {326-334},
	keywords = {Pattern recognition;Vectors;Application software;Boolean
	            functions;Geometry;History},
	doi = {10.1109/PGEC.1965.264137},
}

@article{Fowlkes1983,
	author = {E. B. Fowlkes and C. L. Mallows},
	title = {A Method for Comparing Two Hierarchical Clusterings},
	journal = {Journal of the American Statistical Association},
	volume = {78},
	number = {383},
	pages = {553--569},
	year = {1983},
	publisher = {ASA Website},
	doi = {10.1080/01621459.1983.10478008},
	URL = {
	       https://www.tandfonline.com/doi/abs/10.1080/01621459.1983.10478008
	       },
	eprint = {
	          https://www.tandfonline.com/doi/pdf/10.1080/01621459.1983.10478008
	          },
	abstract = { This article concerns the derivation and use of a measure
	            of similarity between two hierarchical clusterings. The
	            measure, Bk , is derived from the matching matrix, [mij ],
	            formed by cutting the two hierarchical trees and counting the
	            number of matching entries in the k clusters in each tree.
	            The mean and variance of Bk are determined under the
	            assumption that the margins of [mij ] are fixed. Thus, Bk
	            represents a collection of measures for k = 2, …, n – 1. (k,
	            Bk ) plots are found to be useful in portraying the
	            similarity of two clusterings. Bk is compared to other
	            measures of similarity proposed respectively by Baker (1974)
	            and Rand (1971). The use of (k, Bk ) plots for studying
	            clustering methods is explored by a series of Monte Carlo
	            sampling experiments. An example of the use of (k, Bk ) on
	            real data is given. },
}

@article{Rand1971,
	author = {William M. Rand},
	title = {Objective Criteria for the Evaluation of Clustering Methods},
	journal = {Journal of the American Statistical Association},
	volume = {66},
	number = {336},
	pages = {846--850},
	year = {1971},
	publisher = {ASA Website},
	doi = {10.1080/01621459.1971.10482356},
	URL = {
	       https://www.tandfonline.com/doi/abs/10.1080/01621459.1971.10482356
	       },
	eprint = {
	          https://www.tandfonline.com/doi/pdf/10.1080/01621459.1971.10482356
	          },
	abstract = { Many intuitively appealing methods have been suggested for
	            clustering data, however, interpretation of their results has
	            been hindered by the lack of objective criteria. This article
	            proposes several criteria which isolate specific aspects of
	            the performance of a method, such as its retrieval of
	            inherent structure, its sensitivity to resampling and the
	            stability of its results in the light of new data. These
	            criteria depend on a measure of similarity between two
	            different clusterings of the same set of data; the measure
	            essentially considers how each pair of data points is
	            assigned in each clustering. },
}

@article{bouldin1979,
	author = {Davies, David L. and Bouldin, Donald W.},
	journal = {IEEE Transactions on Pattern Analysis and Machine
	           Intelligence},
	title = {A Cluster Separation Measure},
	year = {1979},
	volume = {PAMI-1},
	number = {2},
	pages = {224-227},
	abstract = {A measure is presented which indicates the similarity of
	            clusters which are assumed to have a data density which is a
	            decreasing function of distance from a vector characteristic
	            of the cluster. The measure can be used to infer the
	            appropriateness of data partitions and can therefore be used
	            to compare relative appropriateness of various divisions of
	            the data. The measure does not depend on either the number of
	            clusters analyzed nor the method of partitioning of the data
	            and can be used to guide a cluster seeking algorithm.},
	keywords = {Dispersion;Density measurement;Algorithm design and
	            analysis;Clustering algorithms;Partitioning
	            algorithms;Multidimensional systems;Data analysis;Performance
	            analysis;Humans;Missiles;Cluster;data
	            partitions;multidimensional data analysis;parametric
	            clustering;partitions;similarity measure},
	doi = {10.1109/TPAMI.1979.4766909},
	ISSN = {1939-3539},
	month = {04},
}

@article{rousseuw1987,
	title = {Silhouettes: A graphical aid to the interpretation and
	         validation of cluster analysis},
	journal = {Journal of Computational and Applied Mathematics},
	volume = {20},
	pages = {53-65},
	year = {1987},
	issn = {0377-0427},
	doi = {https://doi.org/10.1016/0377-0427(87)90125-7},
	url = {
	       https://www.sciencedirect.com/science/article/pii/0377042787901257
	       },
	author = {Peter J. Rousseeuw},
	keywords = {Graphical display, cluster analysis, clustering validity,
	            classification},
	abstract = {A new graphical display is proposed for partitioning
	            techniques. Each cluster is represented by a so-called
	            silhouette, which is based on the comparison of its tightness
	            and separation. This silhouette shows which objects lie well
	            within their cluster, and which ones are merely somewhere in
	            between clusters. The entire clustering is displayed by
	            combining the silhouettes into a single plot, allowing an
	            appreciation of the relative quality of the clusters and an
	            overview of the data configuration. The average silhouette
	            width provides an evaluation of clustering validity, and
	            might be used to select an ‘appropriate’ number of clusters.},
}

@article{lloyd1982,
	author = {Lloyd, S.},
	journal = {IEEE Transactions on Information Theory},
	title = {Least squares quantization in PCM},
	year = {1982},
	volume = {28},
	number = {2},
	pages = {129-137},
	abstract = {It has long been realized that in pulse-code modulation
	            (PCM), with a given ensemble of signals to handle, the
	            quantum values should be spaced more closely in the voltage
	            regions where the signal amplitude is more likely to fall. It
	            has been shown by Panter and Dite that, in the limit as the
	            number of quanta becomes infinite, the asymptotic fractional
	            density of quanta per unit voltage should vary as the
	            one-third power of the probability density per unit voltage
	            of signal amplitudes. In this paper the corresponding result
	            for any finite number of quanta is derived; that is,
	            necessary conditions are found that the quanta and associated
	            quantization intervals of an optimum finite quantization
	            scheme must satisfy. The optimization criterion used is that
	            the average quantization noise power be a minimum. It is
	            shown that the result obtained here goes over into the Panter
	            and Dite result as the number of quanta become large. The
	            optimum quautization schemes for2^{b}quanta,b=1,2, \cdots, 7,
	            are given numerically for Gaussian and for Laplacian
	            distribution of signal amplitudes.},
	keywords = {},
	doi = {10.1109/TIT.1982.1056489},
	ISSN = {1557-9654},
	month = {03},
}

@article{shao2016,
	title = {Efficient Leave-One-Out Cross-Validation-based Regularized
	         Extreme Learning Machine},
	journal = {Neurocomputing},
	volume = {194},
	pages = {260-270},
	year = {2016},
	issn = {0925-2312},
	doi = {https://doi.org/10.1016/j.neucom.2016.02.058},
	url = {
	       https://www.sciencedirect.com/science/article/pii/S0925231216003052
	       },
	author = {Zhifei Shao and Meng Joo Er},
	keywords = {Extreme Learning Machine (ELM), Regularized ELM (RELM),
	            Ridge regression, LOO-CV, Leave-One-Out Cross-Validation},
	abstract = {It is well known that the Leave-One-Out Cross-Validation
	            (LOO-CV) is a highly reliable procedure in terms of model
	            selection. Unfortunately, it is an extremely tedious method
	            and has rarely been deployed in practical applications. In
	            this paper, a highly efficient Leave-One-Out Cross-Validation
	            (LOO-CV) formula has been developed and integrated with the
	            popular Regularized Extreme Learning Machine (RELM). The main
	            contribution of this paper is the proposed algorithm, termed
	            as Efficient LOO-CV-based RELM (ELOO-RELM), that can
	            effectively and efficiently update the LOO-CV error with
	            every regularization parameter and automatically select the
	            optimal model with limited user intervention. Rigorous
	            analysis of computational complexity shows that the ELOO-RELM
	            , including the tuning process, can achieve similar
	            efficiency as the original RELM with pre-defined parameter,
	            in which both scale linearly with the size of the training
	            data. An early termination criterion is also introduced to
	            further speed up the learning process. Experimentation
	            studies on benchmark datasets show that the ELOO-RELM can
	            achieve comparable generalization performance as the Support
	            Vector Machines (SVM) with significantly higher learning
	            efficiency. More importantly, comparing to the trial and
	            error tuning procedure employed by the original RELM, the
	            ELOO-RELM can provide more reliable results by the virtue of
	            incorporating the LOO-CV procedure.},
}

@article{pain2020,
	author = {Pain, Oliver and Glanville, Kylie and Hagenaars, Saskia and
	          Selzam, Saskia and Fürtjes, Anna and Coleman, Jonathan and
	          Rimfeld, Kaili and Breen, Gerome and Folkersen, Lasse and Lewis
	          , Cathryn},
	year = {2020},
	month = {12},
	pages = {},
	title = {Imputed Gene Expression Risk Scores: A Functionally Informed
	         Component of Polygenic Risk},
	doi = {10.1101/2020.12.01.369462},
}

@article{skmlearn,
	title = {A scikit-based Python environment for performing multi-label
	         classification},
	author = {Piotr Szymański and Tomasz Kajdanowicz},
	year = {2018},
	eprint = {1702.01460},
	archivePrefix = {arXiv},
	primaryClass = {cs.LG},
	url = {https://arxiv.org/abs/1702.01460},
}

@article{Breiman1996,
	author = {Breiman, Leo},
	title = {Bagging predictors},
	journal = {Machine Learning},
	year = {1996},
	month = {08},
	day = {01},
	volume = {24},
	number = {2},
	pages = {123-140},
	abstract = {Bagging predictors is a method for generating multiple
	            versions of a predictor and using these to get an aggregated
	            predictor. The aggregation averages over the versions when
	            predicting a numerical outcome and does a plurality vote when
	            predicting a class. The multiple versions are formed by
	            making bootstrap replicates of the learning set and using
	            these as new learning sets. Tests on real and simulated data
	            sets using classification and regression trees and subset
	            selection in linear regression show that bagging can give
	            substantial gains in accuracy. The vital element is the
	            instability of the prediction method. If perturbing the
	            learning set can cause significant changes in the predictor
	            constructed, then bagging can improve accuracy.},
	issn = {1573-0565},
	doi = {10.1007/BF00058655},
	url = {https://doi.org/10.1007/BF00058655},
}

@article{Bauer1999,
	author = {Bauer, Eric and Kohavi, Ron},
	title = {An Empirical Comparison of Voting Classification Algorithms:
	         Bagging, Boosting, and Variants},
	journal = {Machine Learning},
	year = {1999},
	month = {07},
	day = {01},
	volume = {36},
	number = {1},
	pages = {105-139},
	abstract = {Methods for voting classification algorithms, such as
	            Bagging and AdaBoost, have been shown to be very successful
	            in improving the accuracy of certain classifiers for
	            artificial and real-world datasets. We review these
	            algorithms and describe a large empirical study comparing
	            several variants in conjunction with a decision tree inducer
	            (three variants) and a Naive-Bayes inducer. The purpose of
	            the study is to improve our understanding of why and when
	            these algorithms, which use perturbation, reweighting, and
	            combination techniques, affect classification error. We
	            provide a bias and variance decomposition of the error to
	            show how different methods and variants influence these two
	            terms. This allowed us to determine that Bagging reduced
	            variance of unstable methods, while boosting methods
	            (AdaBoost and Arc-x4) reduced both the bias and variance of
	            unstable methods but increased the variance for Naive-Bayes,
	            which was very stable. We observed that Arc-x4 behaves
	            differently than AdaBoost if reweighting is used instead of
	            resampling, indicating a fundamental difference. Voting
	            variants, some of which are introduced in this paper,
	            include: pruning versus no pruning, use of probabilistic
	            estimates, weight perturbations (Wagging), and backfitting of
	            data. We found that Bagging improves when probabilistic
	            estimates in conjunction with no-pruning are used, as well as
	            when the data was backfit. We measure tree sizes and show an
	            interesting positive correlation between the increase in the
	            average tree size in AdaBoost trials and its success in
	            reducing the error. We compare the mean-squared error of
	            voting methods to non-voting methods and show that the voting
	            methods lead to large and significant reductions in the
	            mean-squared errors. Practical problems that arise in
	            implementing boosting algorithms are explored, including
	            numerical instabilities and underflows. We use scatterplots
	            that graphically show how AdaBoost reweights instances,
	            emphasizing not only ``hard'' areas but also outliers and
	            noise.},
	issn = {1573-0565},
	doi = {10.1023/A:1007515423169},
	url = {https://doi.org/10.1023/A:1007515423169},
}


@article{Breiman2001,
	author = {Breiman, Leo},
	title = {Random Forests},
	journal = {Machine Learning},
	year = {2001},
	month = {10},
	day = {01},
	volume = {45},
	number = {1},
	pages = {5-32},
	abstract = {Random forests are a combination of tree predictors such
	            that each tree depends on the values of a random vector
	            sampled independently and with the same distribution for all
	            trees in the forest. The generalization error for forests
	            converges a.s. to a limit as the number of trees in the
	            forest becomes large. The generalization error of a forest of
	            tree classifiers depends on the strength of the individual
	            trees in the forest and the correlation between them. Using a
	            random selection of features to split each node yields error
	            rates that compare favorably to Adaboost (Y. Freund {\&} R.
	            Schapire, Machine Learning: Proceedings of the Thirteenth
	            International conference, ***, 148--156), but are more robust
	            with respect to noise. Internal estimates monitor error,
	            strength, and correlation and these are used to show the
	            response to increasing the number of features used in the
	            splitting. Internal estimates are also used to measure
	            variable importance. These ideas are also applicable to
	            regression.},
	issn = {1573-0565},
	doi = {10.1023/A:1010933404324},
	url = {https://doi.org/10.1023/A:1010933404324},
}













@inproceedings{nguyen2013,
	author = {Nguyen, Tan T. and Sanner, Scott},
	title = {Algorithms for direct 0-1 loss optimization in binary
	         classification},
	year = {2013},
	publisher = {JMLR.org},
	abstract = {While convex losses for binary classification are attractive
	            due to the existence of numerous (provably) efficient methods
	            for finding their global optima, they are sensitive to
	            outliers. On the other hand, while the nonconvex 0-1 loss is
	            robust to outliers, it is NP-hard to optimize and thus rarely
	            directly optimized in practice. In this paper, however, we do
	            just that: we explore a variety of practical methods for
	            direct (approximate) optimization of the 0-1 loss based on
	            branch and bound search, combinatorial search, and coordinate
	            descent on smooth, differentiable relaxations of 0-1 loss.
	            Empirically, we compare our proposed algorithms to logistic
	            regression, SVM, and the Bayes point machine showing that the
	            proposed 0-1 loss optimization algorithms perform at least as
	            well and offer a clear advantage in the presence of outliers.
	            To this end, we believe this work reiterates the importance
	            of 0-1 loss and its robustness properties while challenging
	            the notion that it is difficult to directly optimize.},
	booktitle = {Proceedings of the 30th International Conference on
	             International Conference on Machine Learning - Volume 28},
	pages = {III–1085–III–1093},
	location = {Atlanta, GA, USA},
	series = {ICML'13},
}

@inproceedings{macqueen1967,
	title = {Some methods for classification and analysis of multivariate
	         observations},
	author = {MacQueen, J},
	booktitle = {Proceedings of 5-th Berkeley Symposium on Mathematical
	             Statistics and Probability/University of California Press},
	year = {1967},
}

@inproceedings{aggrawal2001,
	author = "Aggarwal, Charu C. and Hinneburg, Alexander and Keim, Daniel
	          A.",
	editor = "Van den Bussche, Jan and Vianu, Victor",
	title = "On the Surprising Behavior of Distance Metrics in High
	         Dimensional Space",
	booktitle = "Database Theory --- ICDT 2001",
	year = "2001",
	publisher = "Springer Berlin Heidelberg",
	address = "Berlin, Heidelberg",
	pages = "420--434",
	abstract = "In recent years, the effect of the curse of high
	            dimensionality has been studied in great detail on several
	            problems such as clustering, nearest neighbor search, and
	            indexing. In high dimensional space the data becomes sparse,
	            and traditional indexing and algorithmic techniques fail from
	            a effciency and/or effectiveness perspective. Recent research
	            results show that in high dimensional space, the concept of
	            proximity, distance or nearest neighbor may not even be
	            qualitatively meaningful. In this paper, we view the
	            dimensionality curse from the point of view of the distance
	            metrics which are used to measure the similarity between
	            objects. We specifically examine the behavior of the commonly
	            used Lknorm and show that the problem of meaningfulness in
	            high dimensionality is sensitive to the value of k. For
	            example, this means that the Manhattan distance metric
	            L(1norm) is consistently more preferable than the Euclidean
	            distance metric L(2norm) for high dimensional data mining
	            applications. Using the intuition derived from our analysis,
	            we introduce and examine a natural extension of the Lknorm to
	            fractional distance metrics. We show that the fractional
	            distance metric provides more meaningful results both from
	            the theoretical and empirical perspective. The results show
	            that fractional distance metrics can significantly improve
	            the effectiveness of standard clustering algorithms such as
	            the k-means algorithm.",
	isbn = "978-3-540-44503-6",
}

@inproceedings{Larracy2021,
	author = {Larracy, Robyn and Phinyomark, Angkoon and Scheme, Erik},
	booktitle = {2021 43rd Annual International Conference of the IEEE
	             Engineering in Medicine \& Biology Society (EMBC)},
	title = {Machine Learning Model Validation for Early Stage Studies with
	         Small Sample Sizes},
	year = {2021},
	volume = {},
	number = {},
	pages = {2314-2319},
	keywords = {Protocols;Costs;Biological system modeling;Machine
	            learning;Feature extraction;Reflection;Planning},
	doi = {10.1109/EMBC46164.2021.9629697},
}
