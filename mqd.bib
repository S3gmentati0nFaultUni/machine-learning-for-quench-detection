@book{slimani2022superconducting,
	title = {Superconducting Materials: Fundamentals, Synthesis and
	         Applications},
	author = {Slimani, Y. and Hannachi, E.},
	isbn = {9789811912115},
	url = {https://books.google.it/books?id=HBVuEAAAQBAJ},
	year = {2022},
	publisher = {Springer Nature Singapore},
}

@book{tsukerman2020compendium,
	title = {Compendium On Electromagnetic Analysis - From Electrostatics To
	         Photonics: Fundamentals And Applications For Physicists And
	         Engineers (In 5 Volumes)},
	author = {Tsukerman, I.},
	isbn = {9789813270183},
	url = {https://books.google.it/books?id=VPgTEAAAQBAJ},
	year = {2020},
	publisher = {World Scientific Publishing Company},
}

@book{fokker1925physica,
	title = {Physica: Nederlandsch tijdschrift voor natuurkunde},
	author = {Fokker, A.D. and Oosterhuis, E. and van der Pol, B.},
	number = {v. 5},
	lccn = {31004996},
	url = {https://books.google.it/books?id=qhUNAQAAIAAJ},
	year = {1925},
	publisher = {M. Nijhoff},
}

@book{fujita-theory-HTS,
	author = {Fujita, Shigeji and Godoy, Salvador},
	year = {2003},
	month = {01},
	pages = {},
	title = {Theory of High Temperature Superconductivity},
	isbn = {978-1-4020-0149-9},
	journal = {Theory of High Temperature Superconductivity: , Fundamental
	           Theories of Physics, Volume 121. ISBN 978-1-4020-0149-9.
	           Kluwer Academic Publishers, 2003},
	doi = {10.1007/0-306-48216-9},
}

@book{ZhouZhi-Hua2021ML,
	abstract = {Machine Learning},
	author = {Zhou, Zhi-Hua},
	address = {Singapore},
	copyright = {These electronic books are licensed by OhioLINK and may be
	             under copyright protection. Please see the Acceptable Use
	             Guidelines for more information, or contact your librarian.},
	edition = {1st Edition 2021},
	isbn = {9811519668},
	keywords = {Computer Science ; Data Mining and Knowledge Discovery ;
	            Machine Learning ; Mathematics of Computing},
	language = {eng},
	publisher = {Springer Singapore},
	title = {Machine Learning},
	year = {2021},
}

@book{Rebala2019,
	author = "Rebala, Gopinath and Ravi, Ajay and Churiwala, Sanjay",
	title = "Machine Learning Definition and Basics",
	bookTitle = "An Introduction to Machine Learning",
	year = "2019",
	publisher = "Springer International Publishing",
	address = "Cham",
	pages = "1--17",
	abstract = "Machine learning algorithms have shown great promise in
	            providing solutions to complex problems. Some of the
	            applications we use every day from searching the Internet to
	            speech recognition are examples of tremendous strides made in
	            realizing the promise of machine learning.",
	isbn = "978-3-030-15729-6",
	doi = "10.1007/978-3-030-15729-6_1",
	url = "https://doi.org/10.1007/978-3-030-15729-6_1",
}

@book{Cunningham2008,
	author = "Cunningham, P{\'a}draig and Cord, Matthieu and Delany, Sarah
	          Jane",
	editor = "Cord, Matthieu and Cunningham, P{\'a}draig",
	title = "Supervised Learning",
	bookTitle = "Machine Learning Techniques for Multimedia: Case Studies on
	             Organization and Retrieval",
	year = "2008",
	publisher = "Springer Berlin Heidelberg",
	address = "Berlin, Heidelberg",
	pages = "21--49",
	abstract = "Supervised learning accounts for a lot of research activity
	            in machine learning and many supervised learning techniques
	            have found application in the processing of multimedia
	            content. The defining characteristic of supervised learning
	            is the availability of annotated training data. The name
	            invokes the idea of a `supervisor' that instructs the
	            learning system on the labels to associate with training
	            examples. Typically these labels are class labels in
	            classification problems. Supervised learning algorithms
	            induce models from these training data and these models can
	            be used to classify other unlabelled data. In this chapter we
	            ground or analysis of supervised learning on the theory of
	            risk minimization. We provide an overview of support vector
	            machines and nearest neighbour classifiers{\textasciitilde}--
	            probably the two most popular supervised learning techniques
	            employed in multimedia research.",
	isbn = "978-3-540-75171-7",
	doi = "10.1007/978-3-540-75171-7_2",
	url = "https://doi.org/10.1007/978-3-540-75171-7_2",
}

@book{gray2011entropy,
	title = {Entropy and Information Theory},
	author = {Gray, R.M.},
	isbn = {9781441979704},
	series = {SpringerLink : B{\"u}cher},
	url = {https://books.google.it/books?id=wdSOqgVbdRcC},
	year = {2011},
	publisher = {Springer US},
}

@book{quinlan2014c4,
	title = {C4.5: Programs for Machine Learning},
	author = {Quinlan, J.R.},
	isbn = {9780080500584},
	series = {Ebrary online},
	url = {https://books.google.it/books?id=b3ujBQAAQBAJ},
	year = {2014},
	publisher = {Morgan Kaufmann},
}

@book{breiman1984classification,
	title = {Classification and Regression Trees},
	author = {Breiman, L.},
	isbn = {9780534980535},
	lccn = {83019708},
	series = {(The Wadsworth statistics / probability series)},
	url = {https://books.google.de/books?id=uxPvAAAAMAAJ},
	year = {1984},
	publisher = {Wadsworth International Group},
}
	
@book{fisher1996learning,
	title = {Learning from Data: Artificial Intelligence and Statistics V},
	author = {Fisher, D. and Lenz, H.J.},
	isbn = {9780387947365},
	lccn = {96011794},
	series = {Artificial intelligence and statistics},
	url = {https://books.google.de/books?id=cmf_l21oje8C},
	year = {1996},
	publisher = {Springer New York},
}

@book{bramer2007principles,
	title = {Principles of Data Mining},
	author = {Bramer, M.},
	isbn = {9781846287664},
	lccn = {2007922358},
	series = {Undergraduate Topics in Computer Science},
	url = {https://books.google.de/books?id=xVW7NslHNHsC},
	year = {2007},
	publisher = {Springer London},
}

@book{learning-with-kernels,
	author = {Schölkopf, Bernhard and Smola, Alexander J.},
	title = {Learning with Kernels: Support Vector Machines, Regularization,
	         Optimization, and Beyond},
	publisher = {The MIT Press},
	year = {2001},
	month = {12},
	abstract = {A comprehensive introduction to Support Vector Machines and
	            related kernel methods.In the 1990s, a new type of learning
	            algorithm was developed, based on results from statistical
	            learning theory: the Support Vector Machine (SVM). This gave
	            rise to a new class of theoretically elegant learning
	            machines that use a central concept of SVMs—-kernels—for a
	            number of learning tasks. Kernel machines provide a modular
	            framework that can be adapted to different tasks and domains
	            by the choice of the kernel function and the base algorithm.
	            They are replacing neural networks in a variety of fields,
	            including engineering, information retrieval, and
	            bioinformatics.Learning with Kernels provides an introduction
	            to SVMs and related kernel methods. Although the book begins
	            with the basics, it also includes the latest research. It
	            provides all of the concepts necessary to enable a reader
	            equipped with some basic mathematical knowledge to enter the
	            world of machine learning using theoretically well-founded
	            yet easy-to-use kernel algorithms and to understand and apply
	            the powerful algorithms that have been developed over the
	            last few years.},
	isbn = {9780262256933},
	doi = {10.7551/mitpress/4175.001.0001},
	url = {https://doi.org/10.7551/mitpress/4175.001.0001},
}

@book{Nishok2024,
	title = {Machine Learning Algorithms},
	author = {Nishok, V.S. and Arunkumar, A. and Murthy, A. and Praveena, S.
	          },
	isbn = {9789348020901},
	url = {https://books.google.it/books?id=COpCEQAAQBAJ},
	year = {2024},
	publisher = {RK Publication},
}

@book{rossi2024,
	title = {High Luminosity Large Hadron Collider, The: New Machine For
	         Illuminating The Mysteries Of The Universe},
	edition = {2nd},
	author = {Rossi, L. and Bruning, O.},
	isbn = {9789811278969},
	series = {Advanced Series On Directions In High Energy Physics},
	year = {2024},
	publisher = {World Scientific Publishing Company},
}

@book{kelley2017-topology,
	title = {General Topology},
	author = {Kelley, J.L.},
	isbn = {9780486820668},
	series = {Dover Books on Mathematics},
	url = {https://books.google.it/books?id=kgxHDgAAQBAJ},
	year = {2017},
	publisher = {Dover Publications},
}

@book{evgrafov2019analytic,
	title = {Analytic Functions},
	author = {Evgrafov, M.A.},
	isbn = {9780486843667},
	series = {Dover Books on Mathematics},
	url = {https://books.google.it/books?id=N8-wDwAAQBAJ},
	year = {2019},
	publisher = {Dover Publications},
}


@inbook{kkt1951,
	author = "Kuhn, Harold W. and Tucker, Albert W.",
	editor = "Giorgi, Giorgio and Kjeldsen, Tinne Hoff",
	title = "Nonlinear Programming",
	bookTitle = "Traces and Emergence of Nonlinear Programming",
	year = "2014",
	publisher = "Springer Basel",
	address = "Basel",
	pages = "247--258",
	abstract = "Linear programming deals with problems such as (see [ 4], [
	            5]): to maximize a linear function {\$}{\$} {\backslash}rm g{
	            \{}x{\}}{\backslash}equiv {\backslash}sum {\{}c{\_}{\{}i{\}}x
	            {\_}{\{}i{\}}{\}} {\backslash}; {\backslash}rm {\{}of{\}} {
	            \backslash}; n {\backslash};{\backslash}rm{\{}real {
	            \backslash}; variables{\}} {\backslash}; x{\_}{\{}1{\}},...,x
	            {\_}{\{}n{\}} {\$}{\$}(forming a vector x) constrained by m +
	            n linear inequalities.",
	isbn = "978-3-0348-0439-4",
	doi = "10.1007/978-3-0348-0439-4_11",
	url = "https://doi.org/10.1007/978-3-0348-0439-4_11",
}

@inbook{metcalf2016-distances,
	author = {Metcalf, Leigh and Casey, William},
	year = {2016},
	month = {12},
	pages = {3-22},
	title = {Metrics, similarity, and sets},
	isbn = {9780128044520},
	doi = {10.1016/B978-0-12-804452-0.00002-6},
}















@article{invention-superconductivity,
	author = {van Delft, Dirk and Kes, Peter},
	title = {The discovery of superconductivity},
	journal = {Physics Today},
	volume = {63},
	number = {9},
	pages = {38-43},
	year = {2010},
	month = {09},
	abstract = {In a triumphant report to the Royal Netherlands Academy of
	            Arts and Sciences (KNAW), Kamerlingh Onnes documented his
	            achievement in great detail. Therefore it is remarkable that
	            reliable details about his serendipitous discovery of
	            superconductivity three years later have been hard to come
	            by. Lack of information has led to speculations about the
	            discovery. In particular, it has perpetuated an apocryphal
	            tale about the role played by a sleepy young apprentice in
	            Kamerlingh Onnes’s lab. That tale was treated as established
	            fact in a September 1996 Physics Today article by Jacobus de
	            Nobel (page 40). There have even been rumors of the possible
	            disappearance of Kamerlingh Onnes’s laboratory notebooks.},
	issn = {0031-9228},
	doi = {10.1063/1.3490499},
	url = {https://doi.org/10.1063/1.3490499},
}

@article{meissner1933,
	author = {Meissner, W. and Ochsenfeld, R.},
	title = {Ein neuer Effekt bei Eintritt der Supraleitf{\"a}higkeit},
	journal = {Naturwissenschaften},
	year = {1933},
	month = {11},
	day = {01},
	volume = {21},
	number = {44},
	pages = {787-788},
	issn = {1432-1904},
	doi = {10.1007/BF01504252},
	url = {https://doi.org/10.1007/BF01504252},
}

@article{london1935,
	title = { The electromagnetic equations of the supraconductor },
	author = { F. London and H. London },
	journal = { Proceedings of The Royal Society A: Mathematical, Physical
	           and Engineering Sciences },
	year = { 1935 },
	publisher = { The Royal Society },
	volume = { 149 },
	pages = { 71-88 },
	number = { 866 },
	doi = { 10.1098/RSPA.1935.0048 },
}

@article{polarization-magnetization,
	authors = {Carlo Andrea Gonano and Riccardo Enrico Zich and Marco
	           Mussetta},
	title = {DEFINITION FOR POLARIZATION P AND MAGNETIZATION M FULLY
	         CONSISTENT WITH MAXWELL'S EQUATIONS},
	volume = {64},
	journal = {Progress In Electromagnetics Research B},
	year = {2015},
	pages = {83-101},
	doi = {10.2528/PIERB15100606},
}

@article{abrikosov-vortices,
	title = {The magnetic properties of superconducting alloys},
	journal = {Journal of Physics and Chemistry of Solids},
	volume = {2},
	number = {3},
	pages = {199-208},
	year = {1957},
	issn = {0022-3697},
	doi = {https://doi.org/10.1016/0022-3697(57)90083-5},
	url = {
	       https://www.sciencedirect.com/science/article/pii/0022369757900835
	       },
	author = {A.A. Abrikosov},
	abstract = {A new explanation is proposed for the magnetic properties of
	            superconducting alloys based on the theory of Ginzburg and
	            Landau.(1) The structure of the penetrating field is found
	            for a superconductor and the relationship between induction
	            and field strength. The findings are compared with
	            experimental results.},
}

@article{mariotto2022,
	doi = {10.1088/1361-6668/ac39e8},
	url = {https://dx.doi.org/10.1088/1361-6668/ac39e8},
	year = {2021},
	month = {11},
	publisher = {IOP Publishing},
	volume = {35},
	number = {1},
	pages = {015006},
	author = {Mariotto, S and Sorbi, M},
	title = {Quench position reconstruction through harmonic field analysis
	         in superconducting magnets},
	journal = {Superconductor Science and Technology},
	abstract = {The performances of superconducting magnets for particle
	            accelerators are limited by instabilities or disturbances
	            which lead to the transition of the superconducting material
	            to the normal resistive state and the activation of the
	            quench protection system to prevent damage to the magnet. To
	            locate the position of the state transition, voltage taps or
	            quench antennas are the most commonly used technologies for
	            their reliability and accuracy. However, during the
	            production phase of a magnet, the number of voltage taps is
	            commonly reduced to simplify the construction process and
	            quench antennas are generally used only for dipoles or
	            quadrupoles to limit the antenna design complexity. To
	            increase the accuracy in the reconstruction of the quench
	            event position, a novel method, suitable for magnets with
	            independent superconducting coils and quench protected
	            without the use of quench heaters, is proposed in this paper.
	            This method, based on standard magnetic measurement
	            techniques for field harmonic analysis, can locate the
	            position of the superconductor transition inside the magnet
	            after the quench event when the magnet has been discharged.
	            Analyzing the not allowed harmonics produced in the field
	            quality at zero current, the position of the quenched coils
	            can be retrieved for any magnet orders without increasing the
	            complexity of the dedicated measurement technique.},
}

@article{mariotto2022-generic,
	author = {Mariotto, Samuele and De Matteis, Ernesto and Prioli, Marco
	          and Sorbi, Malou and Statera, Marco and Valente, Riccardo},
	year = {2022},
	month = {06},
	pages = {1-1},
	title = {Quench Localization in the High Order Corrector Magnets Using
	         the Harmonic Field Method},
	volume = {32},
	journal = {IEEE Transactions on Applied Superconductivity},
	doi = {10.1109/TASC.2022.3158626},
}

@article{Cyrot1973,
	doi = {10.1088/0034-4885/36/2/001},
	url = {https://dx.doi.org/10.1088/0034-4885/36/2/001},
	year = {1973},
	month = {02},
	publisher = {},
	volume = {36},
	number = {2},
	pages = {103},
	author = {M Cyrot},
	title = {Ginzburg-Landau theory for superconductors},
	journal = {Reports on Progress in Physics},
	abstract = {In this review the author describes how the simple
	            Ginzburg-Landau approach lies in the heart of the general
	            theory of superconductors. The reader is introduced to the
	            handling of the theory and to the numerous possibilities of
	            applications. A general free energy functional for a
	            superconductor is given and the different cases where it can
	            be reduced to a Ginzburg-Landau form or to a simple
	            generalization of this form is studied. It is emphasized that
	            applications are not restricted to thermo-dynamical ones as
	            the Ginzburg-Landau approach can be used in the calculation
	            of dissipative phenomena. The possibility of an extension to
	            the time-dependent phenomena is discussed in detail to
	            present the difficult problems which arise in that case. The
	            gapless regime of type II superconductors is given as an
	            example.},
}

@article{diamantini2023typeiiisuperconductivity,
	title = {Type III superconductivity},
	author = {M. C. Diamantini and C. A. Trugenberger and Sheng-Zong Chen
	          and Yu-Jung Lu and Chi-Te Liang and V. M. Vinokur},
	year = {2023},
	eprint = {2303.14673},
	archivePrefix = {arXiv},
	primaryClass = {cond-mat.supr-con},
	url = {https://arxiv.org/abs/2303.14673},
}

@article{pal2010handwritten,
	title = {Handwritten English character recognition using neural network},
	author = {Pal, Anita and Singh, Dayashankar},
	journal = {International Journal of Computer science \&amp;
	           Communication},
	volume = {1},
	number = {2},
	pages = {141--144},
	year = {2010},
}

@article{overfitting-dt-erblin,
	AUTHOR = {Erblin Halabaku, Eliot Bytyçi},
	TITLE = {Overfitting in Machine Learning: A Comparative Analysis of
	         Decision Trees and Random Forests},
	JOURNAL = {Intelligent Automation \& Soft Computing},
	VOLUME = {39},
	YEAR = {2024},
	NUMBER = {6},
	PAGES = {987--1006},
	URL = {http://www.techscience.com/iasc/v39n6/59139},
	ISSN = {2326-005X},
	ABSTRACT = {Machine learning has emerged as a pivotal tool in
	            deciphering and managing this excess of information in an era
	            of abundant data. This paper presents a comprehensive
	            analysis of machine learning algorithms, focusing on the
	            structure and efficacy of random forests in mitigating
	            overfitting—a prevalent issue in decision tree models. It
	            also introduces a novel approach to enhancing decision tree
	            performance through an optimized pruning method called
	            Adaptive Cross-Validated Alpha CCP (ACV-CCP). This method
	            refines traditional cost complexity pruning by streamlining
	            the selection of the alpha parameter, leveraging
	            cross-validation within the pruning process to achieve a
	            reliable, computationally efficient alpha selection that
	            generalizes well to unseen data. By enhancing computational
	            efficiency and balancing model complexity, ACV-CCP allows
	            decision trees to maintain predictive accuracy while
	            minimizing overfitting, effectively narrowing the performance
	            gap between decision trees and random forests. Our findings
	            illustrate how ACV-CCP contributes to the robustness and
	            applicability of decision trees, providing a valuable
	            perspective on achieving computationally efficient and
	            generalized machine learning models.},
	DOI = {10.32604/iasc.2024.059429},
}

@article{platt1998,
	author = {Platt, John},
	year = {1998},
	month = {07},
	pages = {},
	title = {Sequential Minimal Optimization: A Fast Algorithm for Training
	         Support Vector Machines},
	volume = {208},
	journal = {Advances in Kernel Methods-Support Vector Learning},
}

@article{cover1965,
	author = {Cover, Thomas M.},
	journal = {IEEE Transactions on Electronic Computers},
	title = {Geometrical and Statistical Properties of Systems of Linear
	         Inequalities with Applications in Pattern Recognition},
	year = {1965},
	volume = {EC-14},
	number = {3},
	pages = {326-334},
	keywords = {Pattern recognition;Vectors;Application software;Boolean
	            functions;Geometry;History},
	doi = {10.1109/PGEC.1965.264137},
}

@article{Fowlkes1983,
	author = {E. B. Fowlkes and C. L. Mallows},
	title = {A Method for Comparing Two Hierarchical Clusterings},
	journal = {Journal of the American Statistical Association},
	volume = {78},
	number = {383},
	pages = {553--569},
	year = {1983},
	publisher = {ASA Website},
	doi = {10.1080/01621459.1983.10478008},
	URL = {
	       https://www.tandfonline.com/doi/abs/10.1080/01621459.1983.10478008
	       },
	eprint = {
	          https://www.tandfonline.com/doi/pdf/10.1080/01621459.1983.10478008
	          },
	abstract = { This article concerns the derivation and use of a measure
	            of similarity between two hierarchical clusterings. The
	            measure, Bk , is derived from the matching matrix, [mij ],
	            formed by cutting the two hierarchical trees and counting the
	            number of matching entries in the k clusters in each tree.
	            The mean and variance of Bk are determined under the
	            assumption that the margins of [mij ] are fixed. Thus, Bk
	            represents a collection of measures for k = 2, …, n – 1. (k,
	            Bk ) plots are found to be useful in portraying the
	            similarity of two clusterings. Bk is compared to other
	            measures of similarity proposed respectively by Baker (1974)
	            and Rand (1971). The use of (k, Bk ) plots for studying
	            clustering methods is explored by a series of Monte Carlo
	            sampling experiments. An example of the use of (k, Bk ) on
	            real data is given. },
}

@article{Rand1971,
	author = {William M. Rand},
	title = {Objective Criteria for the Evaluation of Clustering Methods},
	journal = {Journal of the American Statistical Association},
	volume = {66},
	number = {336},
	pages = {846--850},
	year = {1971},
	publisher = {ASA Website},
	doi = {10.1080/01621459.1971.10482356},
	URL = {
	       https://www.tandfonline.com/doi/abs/10.1080/01621459.1971.10482356
	       },
	eprint = {
	          https://www.tandfonline.com/doi/pdf/10.1080/01621459.1971.10482356
	          },
	abstract = { Many intuitively appealing methods have been suggested for
	            clustering data, however, interpretation of their results has
	            been hindered by the lack of objective criteria. This article
	            proposes several criteria which isolate specific aspects of
	            the performance of a method, such as its retrieval of
	            inherent structure, its sensitivity to resampling and the
	            stability of its results in the light of new data. These
	            criteria depend on a measure of similarity between two
	            different clusterings of the same set of data; the measure
	            essentially considers how each pair of data points is
	            assigned in each clustering. },
}

@article{bouldin1979,
	author = {Davies, David L. and Bouldin, Donald W.},
	journal = {IEEE Transactions on Pattern Analysis and Machine
	           Intelligence},
	title = {A Cluster Separation Measure},
	year = {1979},
	volume = {PAMI-1},
	number = {2},
	pages = {224-227},
	abstract = {A measure is presented which indicates the similarity of
	            clusters which are assumed to have a data density which is a
	            decreasing function of distance from a vector characteristic
	            of the cluster. The measure can be used to infer the
	            appropriateness of data partitions and can therefore be used
	            to compare relative appropriateness of various divisions of
	            the data. The measure does not depend on either the number of
	            clusters analyzed nor the method of partitioning of the data
	            and can be used to guide a cluster seeking algorithm.},
	keywords = {Dispersion;Density measurement;Algorithm design and
	            analysis;Clustering algorithms;Partitioning
	            algorithms;Multidimensional systems;Data analysis;Performance
	            analysis;Humans;Missiles;Cluster;data
	            partitions;multidimensional data analysis;parametric
	            clustering;partitions;similarity measure},
	doi = {10.1109/TPAMI.1979.4766909},
	ISSN = {1939-3539},
	month = {04},
}

@article{rousseuw1987,
	title = {Silhouettes: A graphical aid to the interpretation and
	         validation of cluster analysis},
	journal = {Journal of Computational and Applied Mathematics},
	volume = {20},
	pages = {53-65},
	year = {1987},
	issn = {0377-0427},
	doi = {https://doi.org/10.1016/0377-0427(87)90125-7},
	url = {
	       https://www.sciencedirect.com/science/article/pii/0377042787901257
	       },
	author = {Peter J. Rousseeuw},
	keywords = {Graphical display, cluster analysis, clustering validity,
	            classification},
	abstract = {A new graphical display is proposed for partitioning
	            techniques. Each cluster is represented by a so-called
	            silhouette, which is based on the comparison of its tightness
	            and separation. This silhouette shows which objects lie well
	            within their cluster, and which ones are merely somewhere in
	            between clusters. The entire clustering is displayed by
	            combining the silhouettes into a single plot, allowing an
	            appreciation of the relative quality of the clusters and an
	            overview of the data configuration. The average silhouette
	            width provides an evaluation of clustering validity, and
	            might be used to select an ‘appropriate’ number of clusters.},
}

@article{lloyd1982-kmeans,
	author = {Lloyd, S.},
	journal = {IEEE Transactions on Information Theory},
	title = {Least squares quantization in PCM},
	year = {1982},
	volume = {28},
	number = {2},
	pages = {129-137},
	abstract = {It has long been realized that in pulse-code modulation
	            (PCM), with a given ensemble of signals to handle, the
	            quantum values should be spaced more closely in the voltage
	            regions where the signal amplitude is more likely to fall. It
	            has been shown by Panter and Dite that, in the limit as the
	            number of quanta becomes infinite, the asymptotic fractional
	            density of quanta per unit voltage should vary as the
	            one-third power of the probability density per unit voltage
	            of signal amplitudes. In this paper the corresponding result
	            for any finite number of quanta is derived; that is,
	            necessary conditions are found that the quanta and associated
	            quantization intervals of an optimum finite quantization
	            scheme must satisfy. The optimization criterion used is that
	            the average quantization noise power be a minimum. It is
	            shown that the result obtained here goes over into the Panter
	            and Dite result as the number of quanta become large. The
	            optimum quautization schemes for2^{b}quanta,b=1,2, \cdots, 7,
	            are given numerically for Gaussian and for Laplacian
	            distribution of signal amplitudes.},
	keywords = {},
	doi = {10.1109/TIT.1982.1056489},
	ISSN = {1557-9654},
	month = {03},
}

@article{shao2016,
	title = {Efficient Leave-One-Out Cross-Validation-based Regularized
	         Extreme Learning Machine},
	journal = {Neurocomputing},
	volume = {194},
	pages = {260-270},
	year = {2016},
	issn = {0925-2312},
	doi = {https://doi.org/10.1016/j.neucom.2016.02.058},
	url = {
	       https://www.sciencedirect.com/science/article/pii/S0925231216003052
	       },
	author = {Zhifei Shao and Meng Joo Er},
	keywords = {Extreme Learning Machine (ELM), Regularized ELM (RELM),
	            Ridge regression, LOO-CV, Leave-One-Out Cross-Validation},
	abstract = {It is well known that the Leave-One-Out Cross-Validation
	            (LOO-CV) is a highly reliable procedure in terms of model
	            selection. Unfortunately, it is an extremely tedious method
	            and has rarely been deployed in practical applications. In
	            this paper, a highly efficient Leave-One-Out Cross-Validation
	            (LOO-CV) formula has been developed and integrated with the
	            popular Regularized Extreme Learning Machine (RELM). The main
	            contribution of this paper is the proposed algorithm, termed
	            as Efficient LOO-CV-based RELM (ELOO-RELM), that can
	            effectively and efficiently update the LOO-CV error with
	            every regularization parameter and automatically select the
	            optimal model with limited user intervention. Rigorous
	            analysis of computational complexity shows that the ELOO-RELM
	            , including the tuning process, can achieve similar
	            efficiency as the original RELM with pre-defined parameter,
	            in which both scale linearly with the size of the training
	            data. An early termination criterion is also introduced to
	            further speed up the learning process. Experimentation
	            studies on benchmark datasets show that the ELOO-RELM can
	            achieve comparable generalization performance as the Support
	            Vector Machines (SVM) with significantly higher learning
	            efficiency. More importantly, comparing to the trial and
	            error tuning procedure employed by the original RELM, the
	            ELOO-RELM can provide more reliable results by the virtue of
	            incorporating the LOO-CV procedure.},
}

@article{lavasa2021,
	author = {Lavasa, Eleni and Giannopoulos, · and Papaioannou, · and
	          Anastasiadis, Anastasios and Daglis, Ioannis and Aran, Angels
	          and Pacheco, Daniel and Sanahuja, Blai and Springer, ©},
	year = {2021},
	month = {07},
	pages = {},
	title = {Assessing the Predictability of Solar Energetic Particles with
	         the Use of Machine Learning Techniques},
	journal = {Solar Physics},
	doi = {10.1007/s11207-021-01837-x},
}

@article{skmlearn,
	title = {A scikit-based Python environment for performing multi-label
	         classification},
	author = {Piotr Szymański and Tomasz Kajdanowicz},
	year = {2018},
	eprint = {1702.01460},
	archivePrefix = {arXiv},
	primaryClass = {cs.LG},
	url = {https://arxiv.org/abs/1702.01460},
}

@article{Breiman1996,
	author = {Breiman, Leo},
	title = {Bagging predictors},
	journal = {Machine Learning},
	year = {1996},
	month = {08},
	day = {01},
	volume = {24},
	number = {2},
	pages = {123-140},
	abstract = {Bagging predictors is a method for generating multiple
	            versions of a predictor and using these to get an aggregated
	            predictor. The aggregation averages over the versions when
	            predicting a numerical outcome and does a plurality vote when
	            predicting a class. The multiple versions are formed by
	            making bootstrap replicates of the learning set and using
	            these as new learning sets. Tests on real and simulated data
	            sets using classification and regression trees and subset
	            selection in linear regression show that bagging can give
	            substantial gains in accuracy. The vital element is the
	            instability of the prediction method. If perturbing the
	            learning set can cause significant changes in the predictor
	            constructed, then bagging can improve accuracy.},
	issn = {1573-0565},
	doi = {10.1007/BF00058655},
	url = {https://doi.org/10.1007/BF00058655},
}

@article{Bauer1999,
	author = {Bauer, Eric and Kohavi, Ron},
	title = {An Empirical Comparison of Voting Classification Algorithms:
	         Bagging, Boosting, and Variants},
	journal = {Machine Learning},
	year = {1999},
	month = {07},
	day = {01},
	volume = {36},
	number = {1},
	pages = {105-139},
	abstract = {Methods for voting classification algorithms, such as
	            Bagging and AdaBoost, have been shown to be very successful
	            in improving the accuracy of certain classifiers for
	            artificial and real-world datasets. We review these
	            algorithms and describe a large empirical study comparing
	            several variants in conjunction with a decision tree inducer
	            (three variants) and a Naive-Bayes inducer. The purpose of
	            the study is to improve our understanding of why and when
	            these algorithms, which use perturbation, reweighting, and
	            combination techniques, affect classification error. We
	            provide a bias and variance decomposition of the error to
	            show how different methods and variants influence these two
	            terms. This allowed us to determine that Bagging reduced
	            variance of unstable methods, while boosting methods
	            (AdaBoost and Arc-x4) reduced both the bias and variance of
	            unstable methods but increased the variance for Naive-Bayes,
	            which was very stable. We observed that Arc-x4 behaves
	            differently than AdaBoost if reweighting is used instead of
	            resampling, indicating a fundamental difference. Voting
	            variants, some of which are introduced in this paper,
	            include: pruning versus no pruning, use of probabilistic
	            estimates, weight perturbations (Wagging), and backfitting of
	            data. We found that Bagging improves when probabilistic
	            estimates in conjunction with no-pruning are used, as well as
	            when the data was backfit. We measure tree sizes and show an
	            interesting positive correlation between the increase in the
	            average tree size in AdaBoost trials and its success in
	            reducing the error. We compare the mean-squared error of
	            voting methods to non-voting methods and show that the voting
	            methods lead to large and significant reductions in the
	            mean-squared errors. Practical problems that arise in
	            implementing boosting algorithms are explored, including
	            numerical instabilities and underflows. We use scatterplots
	            that graphically show how AdaBoost reweights instances,
	            emphasizing not only ``hard'' areas but also outliers and
	            noise.},
	issn = {1573-0565},
	doi = {10.1023/A:1007515423169},
	url = {https://doi.org/10.1023/A:1007515423169},
}


@article{Breiman2001,
	author = {Breiman, Leo},
	title = {Random Forests},
	journal = {Machine Learning},
	year = {2001},
	month = {10},
	day = {01},
	volume = {45},
	number = {1},
	pages = {5-32},
	abstract = {Random forests are a combination of tree predictors such
	            that each tree depends on the values of a random vector
	            sampled independently and with the same distribution for all
	            trees in the forest. The generalization error for forests
	            converges a.s. to a limit as the number of trees in the
	            forest becomes large. The generalization error of a forest of
	            tree classifiers depends on the strength of the individual
	            trees in the forest and the correlation between them. Using a
	            random selection of features to split each node yields error
	            rates that compare favorably to Adaboost (Y. Freund {\&} R.
	            Schapire, Machine Learning: Proceedings of the Thirteenth
	            International conference, ***, 148--156), but are more robust
	            with respect to noise. Internal estimates monitor error,
	            strength, and correlation and these are used to show the
	            response to increasing the number of features used in the
	            splitting. Internal estimates are also used to measure
	            variable importance. These ideas are also applicable to
	            regression.},
	issn = {1573-0565},
	doi = {10.1023/A:1010933404324},
	url = {https://doi.org/10.1023/A:1010933404324},
}

@article{kuvyrkin2022,
	author = {Kuvyrkin, G. N. and Savelyeva, I. Yu. and Zarubin, V. S. and
	          Zimin, V. N.},
	title = {Mathematical model of thermal breakdown of electrical
	         insulation made of polymer composite},
	journal = {Zeitschrift f{\"u}r angewandte Mathematik und Physik},
	year = {2022},
	month = {07},
	day = {26},
	volume = {73},
	number = {5},
	pages = {181},
	abstract = {A nonlinear one-dimensional mathematical model of a thermal
	            breakdown of an electrical insulation having the shape of a
	            hollow circular cylinder caused by the difference of the
	            electric potentials on the cylindrical surfaces is
	            constructed. The cylinder is made of the material that is a
	            composite with a polymer matrix modified by dispersed
	            inclusions improving characteristics of the insulation. The
	            quantitative analysis of the model is carried out for tasks
	            when the density of the heat flow on the inner surface of the
	            cylinder and conditions of convective heat exchange on its
	            outer surface are given. The results of this analysis allow
	            us to determine the possible applications of polymer
	            composites as dielectrics in various high-voltage electrical
	            and electrophysical devices, particularly, as the electrical
	            insulation of conducting strands of high-voltage DC cables.},
	issn = {1420-9039},
	doi = {10.1007/s00033-022-01824-7},
	url = {https://doi.org/10.1007/s00033-022-01824-7},
}

@article{file1963,
	title = {Observation of Persistent Current in a Superconducting Solenoid
	         },
	author = {File, J. and Mills, R. G.},
	journal = {Phys. Rev. Lett.},
	volume = {10},
	issue = {3},
	pages = {93--96},
	numpages = {0},
	year = {1963},
	month = {02},
	publisher = {American Physical Society},
	doi = {10.1103/PhysRevLett.10.93},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.10.93},
}

@article{huebener2019,
	author = {Huebener, R. P.},
	title = {The Abrikosov Vortex Lattice: Its Discovery and Impact},
	journal = {Journal of Superconductivity and Novel Magnetism},
	year = {2019},
	month = {03},
	day = {01},
	volume = {32},
	number = {3},
	pages = {475-481},
	abstract = {Following some prehistory, we discuss the steps leading to
	            the Abrikosov vortex lattice and type-II superconductivity.
	            Dissipation due to flux flow and flux creep represented an
	            important discovery and leads to the strong interest in
	            magnetic flux pinning. High-temperature superconductors and
	            pancake vortices started the new concepts of vortex matter.
	            Nonlinear effects at high vortex velocities, flux-flow
	            instabilities, and some resulting open questions are
	            discussed.},
	issn = {1557-1947},
	doi = {10.1007/s10948-018-4916-0},
	url = {https://doi.org/10.1007/s10948-018-4916-0},
}

@article{hoang2021,
	author = {Hoang, Duc and Boffo, Cristian and Tran, Nhan and Krave,
	          Steven and Kazi, Sujay and Stoynev, Stoyan and Marinozzi,
	          Vittorio},
	journal = {IEEE Transactions on Applied Superconductivity},
	title = {Intelliquench: An Adaptive Machine Learning System for
	         Detection of Superconducting Magnet Quenches},
	year = {2021},
	volume = {31},
	number = {5},
	pages = {1-5},
	abstract = {In superconducting magnets, the irreversible transition of a
	            portion of the conductor to resistive state is called a
	            “quench.” Having large stored energy, magnets can be damaged
	            by quenches due to localized heating, high voltage, or large
	            force transients. Unfortunately, current quench protection
	            systems can only detect a quench after it happens, and
	            mitigating risks in Low Temperature Superconducting (LTS)
	            accelerator magnets often requires fast response (down to
	            ms). Additionally, protection of High Temperature
	            Superconducting (HTS) magnets is still suffering from
	            prohibitively slow quench detection. In this study, we lay
	            the groundwork for a quench prediction system using an
	            auto-encoder fully-connected deep neural network. After
	            dynamically trained with data features extracted from
	            acoustic sensors around the magnet, the system detects
	            anomalous events seconds before the quench in most of our
	            data. While the exact nature of the events is under
	            investigation, we show that the system can “forecast” a
	            quench before it happens under magnet training conditions
	            through a randomized experiment. This opens up the way of
	            integrated data processing, potentially leading to faster and
	            better diagnostics and detection of magnet quenches.},
	keywords = {Superconducting magnets;Magnetoacoustic
	            effects;Training;Magnetomechanical effects;Sensors;Magnetic
	            sensors;Heuristic algorithms;Machine learning;online
	            learning;quench detection;real-time system;superconducting
	            magnets;quench},
	doi = {10.1109/TASC.2021.3058229},
	ISSN = {1558-2515},
	month = {Aug},
}

@article{zhou2021,
	author = {Zhou, Xiao and Shi, Jing and Gong, Kang and Zhu, Changdong and
	          Hua, Jing and Xu, Jun},
	journal = {IEEE Transactions on Applied Superconductivity},
	title = {A Novel Quench Detection Method Based on CNN-LSTM Model},
	year = {2021},
	volume = {31},
	number = {5},
	pages = {1-5},
	abstract = {The quench of high-temperature superconducting (HTS) devices
	            will seriously affect its safety operation. A fast and
	            reliable quench detection method is of great significance for
	            quench protection. In this paper, a quench detection method
	            based on the CNN-LSTM model is proposed. By processing and
	            analyzing the collected voltage signals in real time, the
	            model can realize quench detection instantly and accurately.
	            Furthermore, by adjusting the probability threshold, the
	            sensitivity and the accuracy of the model can be balanced.
	            Based on a series of quench experiments of HTS tapes and
	            derived data, the CNN-LSTM model is trained and tested. The
	            results show that the proposed method is advanced compared
	            with the voltage threshold method.},
	keywords = {Logic gates;Data models;High-temperature
	            superconductors;Feature extraction;Deep
	            learning;Training;Convolution;Quench
	            Detection;High-temperature Superconducting
	            (HTS);CNN;LSTM;Deep Learning},
	doi = {10.1109/TASC.2021.3070735},
	ISSN = {1558-2515},
	month = {08},
}

@article{fang2005,
	author = {Fan, Rong-En and Chen, Pai-Hsuen and Lin, Chih-Jen},
	year = {2005},
	month = {12},
	pages = {1889-1918},
	title = {Working Set Selection Using Second Order Information for
	         Training Support Vector Machines.},
	volume = {6},
	journal = {Journal of Machine Learning Research},
}

@article{retzlaff2024,
	title = {Post-hoc vs ante-hoc explanations: xAI design guidelines for
	         data scientists},
	journal = {Cognitive Systems Research},
	volume = {86},
	pages = {101243},
	year = {2024},
	issn = {1389-0417},
	doi = {https://doi.org/10.1016/j.cogsys.2024.101243},
	url = {
	       https://www.sciencedirect.com/science/article/pii/S1389041724000378
	       },
	author = {Carl O. Retzlaff and Alessa Angerschmid and Anna Saranti and
	          David Schneeberger and Richard Röttger and Heimo Müller and
	          Andreas Holzinger},
	keywords = {Explainable AI, xAI, Post-hoc, Ante-hoc, Explanations,
	            Guideline},
	abstract = {The growing field of explainable Artificial Intelligence
	            (xAI) has given rise to a multitude of techniques and
	            methodologies, yet this expansion has created a growing gap
	            between existing xAI approaches and their practical
	            application. This poses a considerable obstacle for data
	            scientists striving to identify the optimal xAI technique for
	            their needs. To address this problem, our study presents a
	            customized decision support framework to aid data scientists
	            in choosing a suitable xAI approach for their use-case.
	            Drawing from a literature survey and insights from interviews
	            with five experienced data scientists, we introduce a
	            decision tree based on the trade-offs inherent in various xAI
	            approaches, guiding the selection between six commonly used
	            xAI tools. Our work critically examines six prevalent
	            ante-hoc and post-hoc xAI methods, assessing their
	            applicability in real-world contexts through expert
	            interviews. The aim is to equip data scientists and
	            policymakers with the capacity to select xAI methods that not
	            only demystify the decision-making process, but also enrich
	            user understanding and interpretation, ultimately advancing
	            the application of xAI in practical settings.},
}

@article{tang2008-clustering,
	author = {Tang, Bin and Author, Michael and Milios, Evangelos and
	          Heywood, Malcolm},
	year = {2008},
	month = {01},
	pages = {},
	title = {Comparing and Combining Dimension Reduction Techniques for
	         Efficient Text Clustering},
	journal = {Proceedings of the Workshop on Feature Selection for Data
	           Mining},
}

@article{ambrosio1996-mexpansion,
	author = {Ambrosio, G. and Bellomo, G},
	pages = {},
	title = {Magnetic field, multipole expansion and peak field in 2D for
	         superconducting accelerator magnets},
	year = {1996},
	month = {10},
}







@inproceedings{nguyen2013,
	author = {Nguyen, Tan T. and Sanner, Scott},
	title = {Algorithms for direct 0-1 loss optimization in binary
	         classification},
	year = {2013},
	publisher = {JMLR.org},
	abstract = {While convex losses for binary classification are attractive
	            due to the existence of numerous (provably) efficient methods
	            for finding their global optima, they are sensitive to
	            outliers. On the other hand, while the nonconvex 0-1 loss is
	            robust to outliers, it is NP-hard to optimize and thus rarely
	            directly optimized in practice. In this paper, however, we do
	            just that: we explore a variety of practical methods for
	            direct (approximate) optimization of the 0-1 loss based on
	            branch and bound search, combinatorial search, and coordinate
	            descent on smooth, differentiable relaxations of 0-1 loss.
	            Empirically, we compare our proposed algorithms to logistic
	            regression, SVM, and the Bayes point machine showing that the
	            proposed 0-1 loss optimization algorithms perform at least as
	            well and offer a clear advantage in the presence of outliers.
	            To this end, we believe this work reiterates the importance
	            of 0-1 loss and its robustness properties while challenging
	            the notion that it is difficult to directly optimize.},
	booktitle = {Proceedings of the 30th International Conference on
	             International Conference on Machine Learning - Volume 28},
	pages = {III–1085–III–1093},
	location = {Atlanta, GA, USA},
	series = {ICML'13},
}

@inproceedings{macqueen1967-kmeans,
	title = {Some methods for classification and analysis of multivariate
	         observations},
	author = {MacQueen, J},
	booktitle = {Proceedings of 5-th Berkeley Symposium on Mathematical
	             Statistics and Probability/University of California Press},
	year = {1967},
}

@inproceedings{aggrawal2001-curseofdimensionality,
	author = "Aggarwal, Charu C. and Hinneburg, Alexander and Keim, Daniel
	          A.",
	editor = "Van den Bussche, Jan and Vianu, Victor",
	title = "On the Surprising Behavior of Distance Metrics in High
	         Dimensional Space",
	booktitle = "Database Theory --- ICDT 2001",
	year = "2001",
	publisher = "Springer Berlin Heidelberg",
	address = "Berlin, Heidelberg",
	pages = "420--434",
	abstract = "In recent years, the effect of the curse of high
	            dimensionality has been studied in great detail on several
	            problems such as clustering, nearest neighbor search, and
	            indexing. In high dimensional space the data becomes sparse,
	            and traditional indexing and algorithmic techniques fail from
	            a effciency and/or effectiveness perspective. Recent research
	            results show that in high dimensional space, the concept of
	            proximity, distance or nearest neighbor may not even be
	            qualitatively meaningful. In this paper, we view the
	            dimensionality curse from the point of view of the distance
	            metrics which are used to measure the similarity between
	            objects. We specifically examine the behavior of the commonly
	            used Lknorm and show that the problem of meaningfulness in
	            high dimensionality is sensitive to the value of k. For
	            example, this means that the Manhattan distance metric
	            L(1norm) is consistently more preferable than the Euclidean
	            distance metric L(2norm) for high dimensional data mining
	            applications. Using the intuition derived from our analysis,
	            we introduce and examine a natural extension of the Lknorm to
	            fractional distance metrics. We show that the fractional
	            distance metric provides more meaningful results both from
	            the theoretical and empirical perspective. The results show
	            that fractional distance metrics can significantly improve
	            the effectiveness of standard clustering algorithms such as
	            the k-means algorithm.",
	isbn = "978-3-540-44503-6",
}

@inproceedings{Larracy2021,
	author = {Larracy, Robyn and Phinyomark, Angkoon and Scheme, Erik},
	booktitle = {2021 43rd Annual International Conference of the IEEE
	             Engineering in Medicine \& Biology Society (EMBC)},
	title = {Machine Learning Model Validation for Early Stage Studies with
	         Small Sample Sizes},
	year = {2021},
	volume = {},
	number = {},
	pages = {2314-2319},
	keywords = {Protocols;Costs;Biological system modeling;Machine
	            learning;Feature extraction;Reflection;Planning},
	doi = {10.1109/EMBC46164.2021.9629697},
}

@inproceedings{einstein2023,
	author = {J.A. Einstein-Curtis and K.A. Drees and J.P. Edelen and M.C.
	          Kilpatrick and J.S. Laster and R. O’Rourke and M. Valette},
	title = {{Classification and Prediction of Superconducting Magnet
	         Quenches}},
	booktitle = {Proc. 19th Int. Conf. Accel. Large Exp. Phys. Control Syst.
	             (ICALEPCS'23)},
	eventdate = {2023-10-09/2023-10-13},
	pages = {856--859},
	paper = {TUPDP117},
	language = {english},
	keywords = {power-supply, superconducting-magnet, GUI, operation,
	            experiment},
	venue = {Cape Town, South Africa},
	series = {International Conference on Accelerator and Large Experimental
	          Physics Control Systems},
	number = {19},
	publisher = {JACoW Publishing, Geneva, Switzerland},
	month = {02},
	year = {2024},
	issn = {2226-0358},
	isbn = {978-3-95450-238-7},
	doi = {10.18429/JACoW-ICALEPCS2023-TUPDP117},
	url = {https://jacow.org/icalepcs2023/papers/tupdp117.pdf},
	abstract = {{Robust and reliable quench detection for superconducting
	            magnets is increasingly important as facilities push the
	            boundaries of intensity and operational runtime. RadiaSoft
	            has been working with Brookhaven National Lab on quench
	            detection and prediction for superconducting magnets
	            installed in the RHIC storage rings. This project has
	            analyzed several years of power supply and beam position
	            monitor data to train automated classification tools and
	            automated quench precursor determination based on input
	            sequences. Classification was performed using supervised
	            multilayer perceptron and boosted decision tree architectures
	            , while models of the expected operation of the ring were
	            developed using a variety of autoencoder architectures. We
	            have continued efforts to maximize area under the receiver
	            operating characteristic curve for the multiple
	            classification problem of real quench, fake quench, and
	            no-quench events. We have also begun work on long short-term
	            memory (LSTM) and other recurrent architectures for quench
	            prediction. Examinations of future work utilizing more robust
	            architectures, such as variational autoencoders and Siamese
	            models, as well as methods necessary for uncertainty
	            quantification will be discussed. }},
}

@inproceedings{osuna1997,
	author = {Osuna, E. and Freund, R. and Girosi, F.},
	booktitle = {Neural Networks for Signal Processing VII. Proceedings of
	             the 1997 IEEE Signal Processing Society Workshop},
	title = {An improved training algorithm for support vector machines},
	year = {1997},
	volume = {},
	number = {},
	pages = {276-285},
	keywords = {Support vector machines;Quadratic programming;Support vector
	            machine classification;Large-scale systems;Exchange
	            rates;Pattern classification;Classification
	            algorithms;Polynomials;Neural networks;Minimization methods},
	doi = {10.1109/NNSP.1997.622408},
}

@inproceedings{allaoui2020-clustering,
	author = "Allaoui, Mebarka and Kherfi, Mohammed Lamine and Cheriet,
	          Abdelhakim",
	editor = "El Moataz, Abderrahim and Mammass, Driss and Mansouri, Alamin
	          and Nouboud, Fathallah",
	title = "Considerably Improving Clustering Algorithms Using UMAP
	         Dimensionality Reduction Technique: A Comparative Study",
	booktitle = "Image and Signal Processing",
	year = "2020",
	publisher = "Springer International Publishing",
	address = "Cham",
	pages = "317--325",
	abstract = "Dimensionality reduction is widely used in machine learning
	            and big data analytics since it helps to analyze and to
	            visualize large, high-dimensional datasets. In particular, it
	            can considerably help to perform tasks like data clustering
	            and classification. Recently, embedding methods have emerged
	            as a promising direction for improving clustering accuracy.
	            They can preserve the local structure and simultaneously
	            reveal the global structure of data, thereby reasonably
	            improving clustering performance. In this paper, we
	            investigate how to improve the performance of several
	            clustering algorithms using one of the most successful
	            embedding techniques: Uniform Manifold Approximation and
	            Projection or UMAP. This technique has recently been proposed
	            as a manifold learning technique for dimensionality
	            reduction. It is based on Riemannian geometry and algebraic
	            topology. Our main hypothesis is that UMAP would permit to
	            find the best clusterable embedding manifold, and therefore,
	            we applied it as a preprocessing step before performing
	            clustering. We compare the results of many well-known
	            clustering algorithms such ask-means, HDBSCAN, GMM and
	            Agglomerative Hierarchical Clustering when they operate on
	            the low-dimension feature space yielded by UMAP. A series of
	            experiments on several image datasets demonstrate that the
	            proposed method allows each of the clustering algorithms
	            studied to improve its performance on each dataset
	            considered. Based on Accuracy measure, the improvement can
	            reach a remarkable rate of 60{\%}.",
	isbn = "978-3-030-51935-3",
}
